\chapter{Literature Review}

\section{Sign Language Applications of Hand Gesture Recognition in Human Computer Interaction (HCI) Research}

Gesture recognition is a research field of computer science that is explored in a number of fields, including robotics, machine learning and Human Computer Interaction (HCI). Gesture recognition focuses on the computer recognition of expressions or motions by humans including hands, body language and facial expressions. Lately, HCI has gained a lot of research attention utilizing hand gestures \footcite{Chen2003}. There are many applications which employ gestures to control output such as media players, remote controllers, robots and virtual objects or environments \footcite{Mantyjarvi2004}\footcite{Ong2005}. The output of sign language is considered to be ``one of the single most prominent applications of hand gesture recognition''\footcite{Premaratne2010}.

Sign language recognition is a valuable application of gesture recognition research. Researchers have explored the idea that sign language hand gestures can be used to interact with computer interfaces\footcite{Premaratne2010}. The advancements in sensors, accelerometers and infrared cameras further enhanced the accuracy of recognition modules.

Since the 1990s, there has been lots of research into developing technology for sign language users\footcite{Starner1998}. Advancements in hand gesture recognition research has helped improve recognition for sign language assistive technology. Sign language hand gestures can be recognized by processing four patterns: ``hand shape also known as hand configuration, hand movement, orientation and classification.''\footcite{Zhang2009}.

\begin{quote}
  ``Today the focus has shifted again from the mundane use of sign language to the more advanced human machine interaction. This would in effect advance the interactions that disabled people would have with technology as well as make sign languages easily understandable by ordinary users. The technology can also pave way for automatic translation to other languages in other parts of the world making a silent communication revolution for the disability. Yet, the challenges are enormous and the different approaches taken by researchers around the world have shed light on difficulties ahead as well as the progress made so far.''\footcite{Premaratne2010}. 
\end{quote}

In this section I discuss various approaches taken by researchers, over the last two decades, to enhance the accuracy of hand gesture recognition and expand its applications.

I start with an overview of the historical development of hand gesture recognition in HCI research, highlighting the invention of data glove-based control interfaces and how that was eventually combined with computer vision, where gloves used markers and colours for finger tracking rather than sensors, leading up to the glove based systems we know today. 

\section{Overview of Hand Gesture Recognition Research in Human Computer Interaction HCI Since the 1980s}

In this section, I collate a non-exhaustive summary of hand gesture recognition prototypes highlighting elements which relates to my research. 

Humans have always used hand gestures as a natural means of non-verbal communication. The field of Human Computer Interaction (HCI) incorporates extensive literature on research for recognizing hand gestures through machine learning, for the purpose of replacing keyboard and mouse interaction with electronic devices\footcite{Takahashi1992}. For the last few decades, hand gesture recognition research has made significant contributions to interactive human-machine interfaces and virtual environments\footcite{Takahashi1992}. 

Some gesture recognition studies focus on static hand postures\footcite{Klboz2015}, while others analyse dynamic hand motions\footcite{Rigoll1997}. HCI interpretation of gestures require the posture and movement of hands, arms and sometimes other parts of the body to be measurable by the machine\footcite{Pavlovic1997}.

Since the 1980s there have been a number of studies dedicated to developing gesture-based interaction techniques in the domain of HCI\footcite{Rautaray2015}. These studies are mainly classified as glove-based or vision-based\footcite{Rautaray2015}\footcite{Pavlovic1997}.

The first research approach to recognize hand gestures was to measure the bending of finger joints and hand orientation by designing special gloves called ``Data Gloves''\footcite{Rautaray2015}\footcite{Pavlovic1997}\footcite{Takahashi1992}\footcite{Liang1998}. Data Gloves are gloves wired with flex sensors (sensors to measure finger bends and joints angles), accelerometers and gyroscopes which are used to measure hand orientation and direction. Data Gloves have proved to be very reliable in relaying hand gestures position and motion data\footcite{Mitra2007}. However, the multiple wires which connected the gloves to the computer limited users' mobility. This led to the development of a wireless approach to gesture recognition defined as ``vision-based systems''\footcite{Rautaray2015}\footcite{Pavlovic1997}. Vision-based hand gesture recognition systems employed multiple cameras to classify hand gestures but required complex software for image processing to isolate the hand gestures and dealt with finger occlusion\footcite{Shen2012}. 

Before flex sensors was available, researchers used light tubes\footcite{DipietroL.SabatiniA.M.Dario2008}, fibreoptic\footcite{DipietroL.SabatiniA.M.Dario2008} and resistive ink\footcite{LaViola1999} to detect if fingers were flexed or bent. 

The earliest documentation of a sensor-based Data Glove was developed in 1983 by Gary Grimes\footcite{DipietroL.SabatiniA.M.Dario2008} commissioned by ‘‘Digital Entry Data Glove’'. This glove was wired with multiple sensors. Touch and proximity sensors were attached to determine if two fingers were making contact with each other. Flex sensors were placed over the knuckles to measure fingers bending. And a tilt sensor was positioned at the wrist to detect hand orientation. This glove was programmed to recognize 80 ``alphanumeric characters''. Despite the complex circuitry, this glove had low accuracy rates and had heavy wiring. The development of this glove stopped at the proof of concept phase and never made it to commercialisation.

``MIT Data Glove''\footcite{Premaratne2010} was one of the earliest advanced data gloves of the 1980s. It became a commercial product for the gaming industry and was considered a pioneer in HCI research to replace keyboard input. Registered as acceleGlove \footcite{AcceleGlove2016}\footcite{AcceleGlove2017}, the glove was wired with an accelerometer to record hand and finger movement in 3D. acceleGlove has applications in video games, sports training, physical rehabilitation and virtual reality. It is costs between \$1000–\$5000\footnote{Cost is an important consideration for this research. Making an affordable and accessible glove is highlighted as one of the research goals.}.

More data gloves started to appear in the industry, designed for motion capture, music applications and animation. I mention below three examples that made headlines: \textit{CyberGlove II \& III}\footcite{CyberGloveII}, \textit{5DT Data Glove}\footcite{5DTech} and \textit{P5 Glove}\footcite{P5Glove}. These gloves were highly accurate but very expensive and could only be operated by professionals and in a studio setting.

CyberGlove II and CyberGlove III are two generations of data gloves developed by CyberGlove Systems\footcite{CyberGloveII}. They are designed for motion capture for the motion picture, visual effects and animation industries. These gloves are wired with 22 sensors including flex sensors and a WiFi\texttrademark chip to send data wirelessly to a controller computer. 

5DT Data Glove was also designed specifically for motion capture for the motion picture and animation industries\footcite{5DTech}. It is wired with an array of sensors and is Bluetooth\texttrademark enabled to allow the provision of a wireless data transfer system, running in real time\footcite{5DTech}. 

X-IST Data Glove\footcite{XISTDataGlove} is a motion capture glove with touch sensors placed on the fingertips. It was designed to be used for music related applications. It connects to the computer via a USB cable\footcite{XISTDataGlove}. 

A great example of a data glove which replaced keyboard and mouse input for gaming is the P5 Glove developed by MindFlux\footcite{P5Glove}. The P5 Glove is wired with bend sensors and remote tracking technologies. It was designed to be used for interactive 3D and virtual environments in gaming and educational websites\footcite{P5Glove}. 

The P5 Glove was developed by MindFlux\texttrademark as a way to provide a cheaper alternative to many expensive wired gloves available in the market that can be used for gaming\footcite{P5Glove}. The P5 incorporates a flex sensor as well as remote camera tracking technologies. It provides users intuitive interaction with 3D virtual environments; such as games, websites and educational software. Data Gloves have come a long way in employing advanced sensor technology resulting in satisfactory hand gesture recognition output. However, they remain heavy in wiring and are still largely extremely expensive to manufacture. Vision based recognition systems proved to be more convenient in terms of hand gestures\footcite{Lamberti2012}, as they do not constrain the flexibility of hand movements. However, they still retain other issues as described below.

Although my research focusses on sensor-based Data Gloves for hand gesture recognition, I will highlight briefly the history of Vision-based systems, how they works and how they compare to glove-based gesture recognition.

In the early stages of vision based recognition systems, low resolution cameras and limited computer power made it very difficult to isolate gestures. Non-wired coloured gloves were sometimes used to enhance recognition.

With the advancement of video cameras and fast computing, researchers have moved towards developing vision based gesture recognition systems employing real-time vision processing software\footcite{Pavlovic1997}.

The earliest computer vision gesture recognition system emerged in the 1980s\footcite{Premaratne2010}. Camera based recognition had very low accuracy at the time due to low computing power and cameras low resolution. Coloured gloves was then introduced to help the camera in tracking hand gestures\footcite{James1994}. This was a glove developed by MIT Media Lab where the finger tips were marked with coloured LED which created different ``illumination patterns for different gestures''\footcite{Sturman1994} which could then be segmented and interpreted by the computer.

Occlusion resulted in very poor performance of the glove, especially since there were many variations in hand gestures when performed by different users.

Vision-based recognition modules used multiple layers of feature extraction software, skeletal tracking, sample matching and 3D positioning\footcite{Berci2007}. However, researchers were only able to extract static gestures. Extracting dynamic gestures was still not possible with this approach.

More attempts were made at combining coloured gloves cameras for vision based hand gesture recognition. The earliest of which was reported by Davies et al.\footcite{James1994} who used gloves with coloured finger tips combined with with a grey-scale camera. The system was able to determine seven hand gestures and was mainly designed as a proof of concept as opposed to being a commercialisable product.

Although these approaches represented progress, the problem of occlusion remained. It was later addressed specifically by multiple studies. The earliest being in 1996 by Iwai et al.\footcite{Iwai1996} who introduced the decision tree method to allow the computer to recognize different gestures. The results of this study led to the creation of further research, including the first system to be used for virtual reality applications\footcite{Wang2009}

The late 1990s witnessed a shift in approach for vision based hand gesture recognition systems. Researchers were able to develop hand recognition systems which relied on computer vision but that did not require the use of gloves or markers\footcite{Rehg1994}. This was due to improvements in camera technology, resulting in enhanced resolution and also more reliable detection and analysis. Researchers added a second and in some instances, a third camera to improve the recognition of hand gestures\footcite{Gennery1992} \footcite{Darrell1993}. Depth cameras were later introduced and proved to be revolutionary. 

Although these approaches represented progress, the problem of occlusion remained. It was later addressed specifically by multiple studies. The earliest being in 1996 by Iwai et al.\footcite{Iwai1996} who introduced the decision tree method to allow the computer to recognize different gestures. The results of this study led to the creation of further research, including the first to be used for virtual reality applications\footcite{Wang2009}. 

The late 1990s witnessed a shift in approach for vision based hand gesture recognition systems. Researchers were able to develop hand recognition systems which relied on computer vision without gloves or markers\footcite{Rehg1994}. This was due to improvements in camera technology, resulting in enhanced resolution and also more reliable computing power. Researchers added a second and in some instances, a third camera to improve the recognition of hand gestures\footcite{Gennery1992} \footcite{Darrell1993}. Depth cameras were later introduced and proved to be revolutionary. 

For the first time, real-time gestural extraction was demonstrated in 1995\footcite{Bobick1995} \footcite{Utsumi1999} through the use of depth cameras. However, this required a static background to be present behind the subject.  

At the turn of the millennium, vision-based hand-gesture recognition systems were finally able to identify a growing number of gestures in real time, but only for static gestures. This encouraged researchers to combine the new multi-layered gesture recognition software with the latest camera technology; in an attempt to decipher dynamic hand gestures using computer vision\footcite{Starner1995}. 

As hardware technologies improved, high resolution cameras became easily available to researchers and at a low cost, compared to the more expensive versions used in previous vision based studies. As a result, researchers ``devised new ways to rely on feature extraction from the high quality images available instead of sophisticated multi camera system''\footcite{Chen2003}. 

A pioneer study in recognizing dynamic hand gestures using computer vision was done by Chen et al.\footcite{Chen2003} in 2003. The system was designed to recognize dynamic hand gestured in real time against a static background. Recognition accuracy levels were above 90\% in identifying 20 gestures. The system used complex multilayer software employing hand tracking, feature extraction, Hidden Markov Model (HMM) training and gesture recognition\footcite{Chen2003}. 

Many studies followed\footcite{GastaldiG.PareschiA.SabatiniS.SolariF.Bisio2005} \footcite{Premaratne2007} \footcite{Binh2005} \footcite{Binh2006} \footcite{Berci2007} using different approaches for vision based gesture recognition. Results vary but the primary challenge remains in isolating hand gestures from the background and retaining the mobility of the system. 


``The development of the computer vision based gesture recognition will have to go a long way in realizing what has been achieved by glove based systems. No single one prominent strategy in camera setup to feature extraction to classification has been established as the research indicates different trends in myriad of ways. Yet, a powerful application such as sign language stands to challenges the brightest minds to develop the best of approaches in the above areas for a cohesive solution''\footcite{Premaratne2010}. 

It is important to note that computer-vision based hand-gesture recognition will never be mobile as it relies on high resolution cameras and powerful computing to be successful. In this sense, comfort and cost can be overlooked and glove-based hand-gesture recognition have a better chance on multiple counts to serve as a communication tool for modern sign language users. 

I will now narrow down hand gesture recognition to focus on sign language hand-gesture recognition and how it evolved with both vision based and glove bases systems. I will particularly emphasize American Sign Language as it is what I am basing my recognition schemes on, though they could equally be applied to other forms of sign language. I will also show examples of other forms of sign language. 

Sign language hand gesture recognition research builds on the background research summarized above. 


\section{The Development of Sign Language Recognition Systems to Date}

A central goal of Human Computer Interaction research is to explore the use of new types of interfaces that use different kinds of inputs, for example human gesture. By the 1980s, systems had ``..already been developed to react to limited hand gestures, especially in gaming and in consumer electronics control.'' \footcite{Premaratne2007}.

Since then, hand gesture recognition research has been used to attempt to decipher sign language\footcite{Kawai1985}. There are a range of different sign language libraries and types, just like any language and these vary from one region to another.

My research focuses on classifying American Sign Language (ASL) hand gestures using a machine learning software that can then be trained to recognize customized sign language. 

I chose American Sign Language because it is widely used by the hearing impaired and deaf communities in the USA, Canada and many English speaking countries\footcite{Padden2011}. Recent statistics estimate a range of 500,000 people around the world use ASL including immediate family members of speech disabled individuals\footcite{Neidle1998}.

It is important to identify the fundamental features of ASL to accurately address segmentation and classification research queries. Finger spelling, hand orientation, facial expressions and body language are essential elements to be considered (Costello, 2008). A good example for facial expression is raising the eyebrows to indicate a higher pitch or to ask a question. It is also important to consider that some sign language hand gestures are static, while others are dynamic\footcite{Armstrong2009}. 

Different research approaches employ different classification methods. However, they all use a combination of the elements identified above.

Just like hand gesture recognition systems, sign language recognition systems are based on either computer vision or data gloves.

\section{Computer Vision Based Sign Language Recognition Systems}

Computer vision based sign language recognition systems are divided into two categories: static gesture recognition and dynamic gesture recognition\footcite{Waldron1995} systems. Static gesture recognition is designed to classify isolated hand posture whereas dynamic gesture recognition records and processes continuous hand movement. Both static and dynamic hand gesture recognition systems face the challenge of isolating the hand gesture from the background, not to mention incorporating body movement and facial expression for an accurate translation of sign language.

Depth cameras and coloured marker gloves were used to help the computer isolate hand gestures. This proved to be very difficult in non-controlled environments, limiting recognition to labs and research facilities. As a result, computer based recognition systems were never upgraded to become mobile systems. The size and high cost of the equipment also made it difficult to test outside of the lab.  

I mention here, in chronological order, previous research that has attempted to enhance sign language hand gesture recognition using computer vision utilizing the same methods highlighted above in hand gesture recognition research background.

The earliest accurate system was reported in 1988 by researchers Kawai and Tamura of Osaka University. Kawai and Tamura published a study featuring their attempt at machine recognition of Japanese sign language in real-time\footcite{Tamura1988}. In this study, Kawai and Tamura used image processing techniques to recognize 20 Japanese hand gestures. They could isolate hand gestures from the background by ``comparing a grey scale intensity of two consecutive image frames''\footcite{Tamura1988}.

A decade later, in 1995, MIT researchers Starner and Pentland published research on ``dynamic gesture recognition and classification based on coloured gloves and Hidden Makov Model (HMM) classifier''\footcite{Starner1995b}. They reported a 92\% success rate in accurate translation of ASL without explicitly modelling the fingers, rather, by deciphering hand outlines. Recognition models were based on camera tracking of colored gloves. Their system used a limited vocabulary of 40 hand postures\footcite{Starner1995b}. 

In 1997, Grobel and Assan, researchers at Aachen University of Technology in Germany, utilized HHM classifiers for a video based recognition system of Netherlands sign language\footcite{Grobel1997}. They designed a vision-based system to recognize 262 isolated hand postures. Accuracy level was reported at 94\%. They also used coloured gloves but with an improved design to enhance accuracy levels. 

By 2000, most vision based recognition systems incorporated both static and dynamic sign language hand gestures. This was referred to as local (hand posture and location) and global (hand movement and path) information\footcite{Imagawa2000}.

The first research addressing both local and global gesture information was by Imagawa et al. in Japan (Imagawa et al., 2000). They used a clustering technique to layer multiple images of hands extracted from sign language images. Accuracy was recorded at ``...around 94\% which was a significant achievement given that they relied on very low resolution images''\footcite{Premaratne2010}.

In 2004, researchers Vogler and Metasas at the University of Pennsylvania also devised both static and dynamic gesture recognition system for ASL but this time relied on HMM and 3D motion analysis''\footcite{Vogler2004}. Their system was the first to ``...break down the signs into their constituent phonemes, modelling simultaneous events in stochastically independent channels''\footcite{Vogler2004}. They used a vocabulary of 22 signs and three channels to validate their system. Results were satisfactory at 96\%.

Another breakthrough in 2004 was the employment of neural networks to classify sign language hand gestures feature extraction. 

A pioneering approach using neural networks to recognize sign language hand gestures was attempted by Isaacs and Foo in Florida. Similar to Imagawa et al.\footcite{Imagawa2000}, Issac and Foo also used hand images for attempting video-based sign language recognition. However, they utilize a vector to feed a neural network that recognizes the ASL alphabet\footcite{Isaacs2004}. Their system results in 99\% accuracy in the context of finger spelling. They plan to expand recognition models by designing ``algorithms for ASL feature vector recognition''\footcite{Isaacs2004}.

The above studies suggest that sign language hand gesture segmentation has high accuracy results. More recent research has built on this theory combining it with new emerging computational technologies. 

The debut of Kinect had a very strong impact on the sign recognition community. Kinect offered real time tracking in 3D and successful hand gesture isolation. This presented a ``short-cut to real time performance and made recognition possible in different environments''\footcite{Cooper2012}.

A recent study in 2012, conducted by Cooper et al. at University of Surrey presents a what they call a ``sophisticated sign language recognition system based on Kinect''\footcite{Cooper2012}.

For sign language recognition, Cooper et al. used a two-stage recognition system based on linguistic sub-units paired with Kinect 3D hand tracking in real time. The collected data was combined using a sign language classifier. A neural network was then employed to encode the variations in sub-units (Cooper et al., 2012). This approach resulted in recognition rates of 99\% based on a 20 sign multi-user data set and 81\% on a 40 sign test set. 

Cooper et al.’s research is a culmination of all previous research in the field of gesture recognition based on computer vision and is the most comprehensive effort to date. It was published in many machine learning journals and HCI conferences. 

Sign language recognition based on computer vision gives high accuracy rates when it is used with a limited vocabulary of trained signs. As the number of words increase the accuracy rate declines \footcite{Fang2003}. Results vary greatly between different users due to the variation in hand shapes, speed, position and orientation. Sign language libraries are enormous with some signs being very similar and difficult to distinguish. Computers still struggle with isolation, depth, classification and segmentation. Sophisticated software and multi-stage processing is required to recognize sign language. As a result, it is unlikely that these systems can exist yet as accessible mobile devices or become available universally to sign language users and deaf communities. 

It is for these reasons that I opt to exclude vision based recognition systems from my research and emphasize on data glove based sign language recognition systems. 


\section{Data Glove Based Sign Language Recognition Systems} 

Data glove based systems have proven to be more reliable in registering and relaying hand gestures than vision based systems. Data gloves use sensors that can more reliably detect finger flexing, hand movement and orientation\footcite{AnethaK2014} as well as global and local features. Data glove recognition systems are simpler than vision based systems because they don’t have to consider background isolation or hand motion tracking. 

There are many versions of the data glove that translate sign language to text or speech. I mention here the earliest prototypes and how they progressed to the versions we know today. 

One of the earliest attempts to translate sign language hand gestures to speech was Fels and Hinton’s Glove Talk\footcite{Fels1993} in 1992. They used a data glove and a speech synthesizer to translate 66 root words with six different endings and a vocabulary of up to 200 words. 

Their data glove is wired with sensors to collect finger bending data and hand orientation over 16 parameters which is measured every $\frac{1}{60}^{th}$ of a second. The data is then sent through a computer which defines the text and sends it to a speech synthesizer to translate it into human-like speech. The computer starts processing when it detected a motion from the glove. A stop in motion gives the computer a message of the end of the gesture and it stops processing. One of the challenges they faced was adjusting the singing speed to tell the system when to start/stop processing. Another challenge was the response delays due to the fact that three different software were being used at the same time and sharing the same memory. Glove Talk resulted in 1\% incorrect output and 5\% non-identifiable gestures. The system was not tested with different users to observe system adaptation to user variation. 

Their data glove is wired with sensors to collect finger bending data and hand orientation over 16 parameters which is measured every $\frac{1}{60}^{th}$ of a second. The data is then sent through a computer which defines the text and sends it to a speech synthesizer to translate it to speech. The computer starts processing when it detected a motion from the glove. A stop in motion gives the computer a message of the end of gesture and it stops processing. One of the challenges they faced was adjusting the singing speed to tell the system when to start/stop processing. Another challenge was the response delays due to the fact that three different software were being used at the same time and sharing the same memory. Glove Talk resulted in 1\% incorrect output and 5\% non-identified gesture. The system was not tested with different users to observe system adaptation to user variation. 


In 1998, building on Fel’s and Hinton\footcite{Fels1993} Glove Talk system, researchers Liang and Ouhyoung\footcite{Liang1998} from National Taiwan University were able to interpret Taiwanese sign language in real time using a data glove and HHM. They first solved the end-point detection problem - a major challenge for Glove Talk - by creating a threshold for gesture time variance. They classified recognition models based on four gesture parameters: ``posture, position, orientation and motion''\footcite{Liang1998}. Their prototype system was programmed to recognize a vocabulary of 250 words with an accuracy rate of 80.04\%
In 2003, building on both Fel’s and Hinton\footcite{Fels1993} and Liang and Ouhyoung‘s\footcite{Liang1998} research, Fang et al.\footcite{Fang2003} attempted to develop an advanced sign language recognition system by improving processing speed for a large vocabulary of sign language based on hierarchical decision trees. They acknowledged that output delays were due to the systems being programmed to recognize more words and so their proposal helped the computer prioritize which clusters to look through first. Fang et al‘s research addressed and solved a major challenge in previous data gloves - how to reduce recognition time without the loss of accuracy. Their testing results show processing speed was 11 times faster than previous systems and was able to process a vast vocabulary of 5113 words.

In 2011, a different approach to sign language recognition systems was proposed by Oz and Leu\footcite{Oz2011} who used a motion tracker, an artificial neural network and a sensor glove to translate ASL to speech. Three sets of data was collected and aligned for an improved classification of sign language. Finger and hand shape data was collected from the sensory glove. Hand motion data was collected from the motion tracker. Both data were then classified by the artificial neural network. Gestures feature extraction was continuously being performed in real time. The system was trained to recognize 50 ASL with accuracy results of 90\%. 


\section{Current Academic Projects and Early Prototypes of Sign Language Data Gloves}

Sign language recognition technology is currently being developed by many research teams at universities and digital health labs. Recognition systems are still based on either computer vision or data gloves. However, researchers continue to explore solutions that bring them closer to producing a reliable system which could be integrated into the speech-disabled community and enable them to express themselves more naturally.

In this section I mention some of the most significant academic research projects in this field and highlight the features and limitations of this work. Many of these projects build on the literature mentioned above and show great promise but have not yet moved beyond the research phase to testing or production.

Perhaps one of the earliest academic debuts of a working sign language data glove was AcceleGlove, developed by researcher Jose Hernandez-Rebollar at George Washington University in 2003\footcite{AcceleGlove2016}. AcceleGlove was presented as an experimental device that translated the hand gestures and body language of American Sign Language (ASL) into spoken words. AcceleGlove was perceived as a wearable computer with very small electric circuits which was considered revolutionary at the time. AcceleGlove is a right-hand glove with two small armbands, for the wrist and the upper arm. The glove is wired with sensors and a micro-controller attached to the wrist, mapping the placement and movement of the arm and fingers. The collected data from the sensors is processed by the computer and converted into speech spoken out through a speaker or text displayed on a computer screen. This single glove can produce up to 200 words which could be signed using only one hand and a few expressions. As for accuracy, Jose Hernandez-Rebollar stated that ``the device usually is accurate, though the precision declines with complicated movements; for example, words that start with the same hand movement or orientation'' \footcite{AcceleGlove2016}. This was one of the most powerful data gloves in terms of output to be published in 2003. However, the processing happens on the computer itself as well as the text display, so AcceleGlove could not operate as a mobile device\footcite{AcceleGlove2016}. 


In 2012, a data glove was designed and programmed by two Ukrainian students to translate sign language into speech. The glove was called Enable Talk and took part in a competition organized by Microsoft in which it got the first prize3. Enable Talk is fitted with ``flex sensors, touch sensors, gyroscopes and accelerometers, as well as some solar cells to increase battery life''\footcite{EnableTalk}. The glove has a system that can translate sign language into text and then into spoken words using a text-to-speech engine. The whole system then connects to a smartphone over Bluetooth\texttrademark\footcite{EnableTalk}. A major drawback is that the Enable Talk system mostly uses Microsoft technology and is not compatible with any other platform.

The team has built a number of prototypes and claim to have tested them with sign language-users in the Ukraine, although no documentation of usability studies have been shared or published. Enable Talk would have been highly competitive if introduced into the market because it was set to cost under \$100 and also promised to come equipped with a software the enables the users to teach the system new gestures and eventually build a library of custom gestures\footcite{EnableTalk}. However, no further research has been done on this project since 2012 and it did not move into production\footcite{EnableTalk}. 

In 2013, inspired by Jeremy Blum‘s innovation, the Sudo Glove\footcite{Blum2012}, which is a sensor data glove for non sign language applications, Roman Rozak set out to create a device that could utilise the same technology (flex sensors, accelerometer and microcontroller) while accomplishing a completely different task: translating sign language into text and speech. Roman Kozak, a high school student at the time, is probably the youngest programmer who designed a glove-based sign language translator. He was also the first to program an Arduino to read analog data from flex sensors and outputs them as letters matching the sensor data with a serious of if-statements. The accuracy levels were outstanding. However, it was still limited to letters, specifically, one letter at a time. Letters were not aligned to form sentences. Rozak stated that ``distinguishing between similar sign language gestures was very challenging''\footcite{Rozak}. Processing and display of letters happened on a computer screen or a smart device tablet sent wirelessly via bluetooth. Kozak has now stopped working on this glove and instead moved on to create a similar innovation with different technology\footcite{Rozak}.

In 2014, Gesture Glove\footcite{CornellGlove} was another project that generated significant press attention. It was designed by two groups of students at Cornell University who have developed a different version of a glove which translated sign language to speech. Designed and built to be worn on the right hand, this glove used a machine learning algorithm to translate sign language into words\footcite{CornellGlove}. The glove hardware is very similar to previous gloves, including most data gloves\footcite{Premaratne2013a}. It consisted of five flex sensors, a gyroscope and a microcontroller. The incoming data from the sensors are sent serially to a computer to be analysed in conjunction with a Python script. By collecting a moderate amount of this data for each letter or word and feeding it into a machine learning algorithm, it can train over this dataset and learn to associate a given hand gesture with its corresponding sign \footcite{CornellGlove}. It is interesting that the glove continuously learns from the user. However, there are important lack of accessibility features to discuss about this glove. First, much of the computation happens on the computer and not the glove itself, which makes the glove heavily reliant on a computer to operate. Secondly, from an accuracy point of view, in some cases, the change in the resistance from the flex sensor will be negligible and the algorithm may be unable to discern the difference between these signs. Thirdly, this glove is only programmed to recognize and output letters, which is not necessarily practical for sign language users. The hardware is bulky at this early prototype state, making it difficult to wear. 


Also in 2014, Anetha K, assistiant professor at the Institute of Technology, Coimbatore in India, developed a sign language recognition data glove called Hand Talk (Anetha K, 2014). Hand Talk uses artificial neural network (ANN) to translate the American Sign Language (ASL) alphabets into text and sound. The glove circuit consisted of a controller unit, text to speech conversion module and a LCD display. The glove itself uses wired with flex sensors, a 3-axis accelerometer and sEMG sensors to capture gestures\footcite{AnethaK2014}. Just like previous data gloves discussed above \footcite{AcceleGlove2016} \footcite{EnableTalk} \footcite{Rozak} \footcite{CornellGlove}, the flex sensor produces the change in resistance value depending on the degree of bend in each finger. The corresponding hand movement and orientation is reported by the tri-axial accelerometer. A novel aspect of this technology is its use of sEMG sensors, which are used to measure the muscle activity of the hand while performing gestures in terms of electrical signals. The recognized gesture are then converted and displayed as corresponding text and speech using a text to speech conversion module\footcite{AnethaK2014}. This glove builds on all previously discussed glove prototypes. Testing for Hand Talk was published based on its ASL alphabet output only, which again makes it not necessarily practical for sign language users. Hand Talk hardware is not wearable. Output relied on a computer to display letters and to produce sound\footcite{AnethaK2014}. 


The latest and most recent sign language translation data glove prototype was developed by researchers at Mexico's National Polytechnic Institute (IPN). This prototype, created by Miguel Felix Mata and Helena Luna Garcia, senses hand movements of the user and identifies them with the 26 letters of the English alphabet\footcite{MexicoPoly}. Once the message reaches the device, it plays a voice. Listeners can then understand what their differently-abled companion or acquaintance is trying to say. Presently, the glove can only read letters of the ASL alphabet, but the researchers indicate that Mexican sign language support is a target feature. Smart textiles with conductive features have been used to detect if the fingers are open or closed. A combination of nylon and polyester was used to support the embedded hardware and give the glove better manoeuvrability. ``Words and phrases are transmitted by Bluetooth\texttrademark to a mobile device with a preloaded application that displays and reads the signs,``\footcite{MexicoPoly} Luna said. This by far is the only sign language data glove that has paid attention to glove materials, appearance and durability. This is also the only glove which designed an application for smart devices to pair with the glove for output. The application is available on the Android platform as Glove Translator but needs the glove to work. The main drawbacks of this prototype are the size of the glove and the output mode. It depends on the app for output and cannot operate as a stand-alone glove. This glove prototype is awaiting patent and manufacturing\footcite{MexicoPoly}.

In summary, all data gloves described above are single gloves, specifically right hand gloves, they use ASL based libraries to process alphabet letters, they all use an external device for output and some of them are extremely bulky and non-wearable. 

My goal is to enhance performance and increase accuracy in comparison with existing data gloves, by making a stand-alone data glove which is accessible, universal and wearable. 

Rather than output single letters, to aim for the glove to process words to form sentences and program a software which can help structure grammatically accurate sentences derived from action words. Although sign language users only sign action words and don’t form full sentences when they sign, the person receiving the speech is costumed to communicating in full sentences. 

Instead of relying on a smart device or a computer, to design a wireless and stand-alone data glove with all required hardware for output is embedded in the design of the glove and powered with a battery. The challenge for this approach is to reduce hardware to make it wearable. 
Consider to pair one right hand glove with a left hand glove and integrate the signals from both gloves to form one more accurate output
In order to make this glove universal, pair it with a translation API to output the signs in different languages even though the sign language used is American.

Considering the vast variations in sign language libraries, to equip with machine learning software to allow each user to train the glove using customized hand gestures. This is mostly useful for Autism users of the Makaton sign language library\footcite{Makaton}. 

To design a mobile app to sync with the glove for training new gestures and building a customized library. The app could be used to set the speech language or connect to the internet for future upgrades.

To make this glove accessible it must be affordable by trying to reduce hardware and replace it with software when possible and also integrating all parts onto one circuit board. 

To make this glove wearable, smart textiles must be utilized and a customized glove design pattern to house all hardware in correspondence with hand and arm ergonomics. Fabric used should be water resistant, machine washable, fire resistant and employ safety measures to insulate the electrical circuit from contact with skin. 

Explore the possibility to create a two-way communication scheme where this glove enables both participants in a conversation to understand each other. This could be done by adding a microphone and a speech to text for the reverse side of the conversation. 