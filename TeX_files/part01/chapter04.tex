4. Evaluation Methods 

Evaluation methods will be divided into two overlapping sets\footcite{Fallman2005}: 
a) Full-scale evaluation studies, and b) Formative evaluation and iterative testing.   

4.1 Full-Scale Evaluation Studies 

HCI studies have used full scale evaluation to compare the performance of different systems\footcite{Wania2006}.  Full scale evaluations are also known to have been used to examine specific features of existing systems for the purpose of further development.  In full scale evaluation studies “A group of representative subjects are recruited to learn and use each of the systems and compare them on a pertinent set of performance measures”\footcite{Cox2008}.
For my proposed data glove prototypes, I will conduct a series of in-depth longitudinal case studies with two groups of users: adults with speech disabilities and children with non-verbal autism. The aim of the studies is to know what will happen to real users over the period of time they will actually use my proposed design of the data glove. These studies will only be feasible by doing direct experiments with real users participating on a full time basis for six months in each comparison group. There are several points to consider for the testing rounds:
•	Due to the nature of the participants, it is not feasible to conduct studies in a group setting or with big numbers of participants. 
•	One-on-one time will be needed with study participants to train them on how to use the new technology, keeping in mind that disabilities will vary between users. 
•	It is common for testing with participants who have disabilities to gain feedback through a care giver, a therapist or a family member\footcite{Lazar2010}

Evaluation criteria will be classified under two main categories:
Performance Metrics: Isolating performance features and setting them as evaluation criteria is key to identifying why a system works better than another.  One proposal\footcite{Roberts1983} is to use a set of "benchmark" tests that are chosen to represent the important functions performed with a system. 
Usability issues: In users’ feedback I will be keen to observe and discover possible trouble-spots in the use of the prototypes, so that solutions can be proposed in the next cycle of prototype design\footcite{Klasnja2011}. To be valuable, evaluations of this kind must look at the details of use (time, errors, user reactions) for isolated functions rather than overall performance. Lessons learned from such studies provide important foundation for the development of future systems designs\footcite{Cox2008}.

4.2 Formative Evaluation and Iterative Testing

Cost is a fundamental factor in this research. Therefore, it is important to justify why I intend to conduct multiple rounds of prototype building and testing.  Designing multiple prototypes each performing an isolated task and testing this particular feature is more effective than prototyping a fully executed system and testing multiple features at once. 
 “The best strategy for good design is to try various options (suggested, of course, by experience with previous similar systems, guidelines, and available principles), test them, and be guided by the failures, successes and comments garnered in watching their use, redesign trying new options, and iterate. This is called formative evaluation or developmental evaluation. The idea is simple enough. The barriers to its more frequent use are largely lack of will (organizational resistance), lack of time, or lack of ingenuity”\footcite{Dix2004}. 
It is documented in previous HCI research\footcite{Cox2008}\footcite{Lazar2010} that formative testing can be both extremely effective and quite economical.  Although a single test is not sufficient, multiple iterations of the whole system are not required to evaluate it. “There are many reports in the literature, of dramatic improvements in usability in cases where two or three iterations were made on each important interface design problem, each requiring about a dozen hours of human testing and an equivalent amount of reprogramming”\footcite{Georges2004}.
HCI researchers\footcite{Cox2008}\footcite{Lazar2010} have strongly recommended that user testing begin as early in the development cycle as possible, so that improvements can be made before design processes and coding become complex. For this to become feasible, it is advised to keep the system development flexible and easily modified to be able to conduct continuous user-testing. This is known as “rapid prototyping, and consists of first developing a system specifically designed to be easily modifiable”\footcite{Wania2006}.  This is done through segmenting performance and postponing the launch of the full system to a later stage in the study\footcite{Dix2004}\footcite{Georges2004}.
An exemplary case study and rational account of the iterative testing and rapid prototyping approach is given in an article by Good et al.\footcite{Cox2008} in which they describe the process as "User derived interface design." 

In this way, a series of prototypes of the data glove will be designed and a chapter will be dedicated to prototyping: design and programming, following the same structure illustrated in figure 2.  First prototype will be a proof of concept, to prove that the system works with minimal hardware and software.  Testing will be conducted and feedback will be fed back into the design loop of the next prototype.  The consequent prototypes’ features will be upgraded gradually based on usability testing, always considering the four main elements of this research: affordable, accessible, universal and most importantly effective in facilitating daily communication between speech disabled individuals and the public. 

As an example, in designing an interface for a prototype voice store and forward system, a first attempt-by an expert human factors team at a set of user procedures produced around 50\% unrecoverable errors in attempts to use the service. After four weeks of testing and three revisions in the protocol, field tests found the procedure to result in less than one error for every hundred uses\footnote{Riley, cited\cite{Cox2008}}. The voice message system demonstrated by IBM at the 1984 Olympics in Los Angeles\footnot{Gould and Boise, cited\cite{Cox2008}}  was developed by a team of programmers and behavioural scientists who continuously tried new versions of the system and its protocol and made revisions for several months.  Despite what would ordinarily be considered a rather small-scale development effort, usability in the initial full-scale trial was extraordinarily good. The development of the much acclaimed user interface for the Apple Lisa computer (including design lessons later incorporated into the MacIntosh) was accomplished by almost continuous formative testing during system and interface development. In this case the testing was done by the manager of the interface programming group himself\footnote{Tesler, cited\cite{Cox2008}}. The tests were relatively informal. Tesler selected a particular issue, for example where to put an "exit" icon on the screen, for semi-formal evaluation, (i.e. for some subjects it was in one place and for others in another), for each small experiment. Then he would have a handful of subjects try each of the two options. Most of the gain was not, however, from the comparison of the options but merely from observing the difficulties experienced by the users, and from the participants' comments and suggestions. According to Tesler the formal comparison served primarily to help in the discipline of systematizing observations. Difficulties were then either taken back to the design team for immediate alterations and retest, placed on a wish list for later solution, or ignored for practical reasons. Iterating this step every time an interesting design question arose, and after every significant milestone in the interface development, required running only about two dozen subjects per week through trials of the system, and caused almost no delay in the total development process since the fixes were made concurrently with the normal course of programming. This whole procedure strikes me as exemplary, as do the somewhat more elaborate and ingenious techniques utilized by Gould., Boies, Levy, Richards and Schoonard\footcite{Cox2008}.

