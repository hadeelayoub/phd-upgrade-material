@article{Fels1993,
author = {Fels, S Sidney and Hinton, Geoffrey E},
issn = {1045-9227},
journal = {IEEE transactions on Neural Networks},
number = {1},
pages = {2--8},
publisher = {IEEE},
title = {{Glove-talk: A neural network interface between a data-glove and a speech synthesizer}},
volume = {4},
year = {1993}
}
@inproceedings{Premaratne2013,
author = {Premaratne, Prashan and Yang, Shuai and Zou, ZhengMao and Vial, Peter},
booktitle = {International Conference on Intelligent Computing},
pages = {509--514},
publisher = {Springer},
title = {{Australian sign language recognition using moment invariants}},
year = {2013}
}
@inproceedings{Klasnja2011,
author = {Klasnja, Predrag and Consolvo, Sunny and Pratt, Wanda},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
isbn = {1450302289},
pages = {3063--3072},
publisher = {ACM},
title = {{How to evaluate technologies for health behavior change in HCI research}},
year = {2011}
}
@inproceedings{Wania2006,
author = {Wania, Christine E and Atwood, Michael E and McCain, Katherine W},
booktitle = {Proceedings of the 6th conference on Designing Interactive systems},
isbn = {1595933670},
pages = {90--98},
publisher = {ACM},
title = {{How do design and evaluation interrelate in HCI research?}},
year = {2006}
}
@inproceedings{Fallman2005,
author = {Fallman, Daniel and Waterworth, John},
booktitle = {Workshop Paper, CHI},
pages = {2--7},
title = {{Dealing with user experience and affective evaluation in hci design: A repertory grid approach}},
year = {2005}
}
@inproceedings{Bevan1999,
author = {Bevan, Nigel and Curson, Ian},
booktitle = {CHI'99 Extended Abstracts on Human Factors in Computing Systems},
isbn = {1581131585},
pages = {137--138},
publisher = {ACM},
title = {{Planning and implementing user-centred design}},
year = {1999}
}
@book{Rogers2011,
author = {Rogers, Yvonne and Sharp, Helen and Preece, Jenny},
isbn = {0470665769},
publisher = {John Wiley {\&} Sons},
title = {{Interaction design: beyond human-computer interaction}},
year = {2011}
}
@inproceedings{Zimmerman:2007:RTD:1240624.1240704,
address = {New York, NY, USA},
author = {Zimmerman, John and Forlizzi, Jodi and Evenson, Shelley},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
doi = {10.1145/1240624.1240704},
isbn = {978-1-59593-593-9},
keywords = { design, design method, design theory, interaction design, interaction design research, research through design, wicked problems,HCI research},
pages = {493--502},
publisher = {ACM},
series = {CHI '07},
title = {{Research Through Design As a Method for Interaction Design Research in HCI}},
url = {http://doi.acm.org/10.1145/1240624.1240704},
year = {2007}
}
@inproceedings{ChristopherFrauenberger2015,
address = {Lisbon, Portugal},
author = {{Christopher Frauenberger}},
booktitle = {17th International ACM SIGACCESS Conference on Computers {\&}{\#}38; Accessibility{\textless}/em{\textgreater} (ASSETS '15)},
keywords = {critical realism,disability studies,philosophy of science},
pages = {89--96},
publisher = {ACM},
title = {{Disability and Technology: A Critical Realist Perspective}},
url = {http://doi.acm.org/10.1145/2700648.2809851},
year = {2015}
}
@online{WikiASL,
booktitle = {Wikipedia},
title = {{ASL}},
url = {https://en.wikipedia.org/wiki/American{\_}Sign{\_}Language}
}
@article{Roberts1983,
author = {Roberts, Teresa L and Moran, Thomas P},
issn = {0001-0782},
journal = {Communications of the ACM},
number = {4},
pages = {265--283},
publisher = {ACM},
title = {{The evaluation of text editors: methodology and empirical results.}},
volume = {26},
year = {1983}
}
@article{Landauer1988,
author = {Landauer, Thomas K},
journal = {Handbook of human-computer interaction},
pages = {905--928},
publisher = {Elsevier},
title = {{Research methods in human-computer interaction}},
year = {1988}
}
@article{Shen2012,
author = {Shen, Xiaohui and Hua, Gang and Williams, Lance and Wu, Ying},
issn = {0262-8856},
journal = {Image and Vision Computing},
number = {3},
pages = {227--235},
publisher = {Elsevier},
title = {{Dynamic hand gesture recognition: An exemplar-based approach from motion divergence fields}},
volume = {30},
year = {2012}
}
@article{Mitra2007,
author = {Mitra, Sushmita and Acharya, Tinku},
issn = {1094-6977},
journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
number = {3},
pages = {311--324},
publisher = {IEEE},
title = {{Gesture recognition: A survey}},
volume = {37},
year = {2007}
}
@article{Rautaray2015,
author = {Rautaray, Siddharth S and Agrawal, Anupam},
issn = {0269-2821},
journal = {Artificial Intelligence Review},
number = {1},
pages = {1--54},
publisher = {Springer},
title = {{Vision based hand gesture recognition for human computer interaction: a survey}},
volume = {43},
year = {2015}
}
@inproceedings{Rigoll1997,
author = {Rigoll, Gerhard and Kosmala, Andreas and Eickeler, Stefan},
booktitle = {International Gesture Workshop},
pages = {69--80},
publisher = {Springer},
title = {{High performance real-time gesture recognition using hidden markov models}},
year = {1997}
}
@online{Blum2012,
author = {Blum, Jeremy},
title = {{Sudo Glove}},
url = {http://www.jeremyblum.com/portfolio/sudoglove-hardware-controller/},
urldate = {2017-05-12},
year = {2012}
}
@article{Klboz2015,
author = {Kılıboz, Nurettin {\c{C}}ağrı and G{\"{u}}d{\"{u}}kbay, Uğur},
issn = {1047-3203},
journal = {Journal of Visual Communication and Image Representation},
pages = {97--104},
publisher = {Elsevier},
title = {{A hand gesture recognition technique for human–computer interaction}},
volume = {28},
year = {2015}
}
@inproceedings{Bobick1995,
author = {Bobick, Aaron F and Wilson, Andrew D},
booktitle = {Computer Vision, 1995. Proceedings., Fifth International Conference on},
isbn = {0818670428},
pages = {382--388},
publisher = {IEEE},
title = {{A state-based technique for the summarization and recognition of gesture}},
year = {1995}
}
@article{Takahashi1992,
author = {Takahashi, Tomoichi and Kishino, Fumio},
file = {:Users/hadeelayoub/Desktop/1-s2.0-S104732031500022X-main.pdf:pdf},
issn = {1520-684X},
journal = {Systems and Computers in Japan},
number = {3},
pages = {38--48},
publisher = {Wiley Online Library},
title = {{A hand gesture recognition method and its application}},
volume = {23},
year = {1992}
}
@article{Baudel1993,
author = {Baudel, Thomas and Beaudouin-Lafon, Michel},
issn = {0001-0782},
journal = {Communications of the ACM},
number = {7},
pages = {28--35},
publisher = {ACM},
title = {{Charade: remote control of objects using free-hand gestures}},
volume = {36},
year = {1993}
}
@article{Pavlovic1997,
author = {Pavlovic, Vladimir I and Sharma, Rajeev and Huang, Thomas S},
file = {:Users/hadeelayoub/Desktop/10.1.1.53.6402.pdf:pdf;:Users/hadeelayoub/Desktop/pavlovic97pami.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on pattern analysis and machine intelligence},
number = {7},
pages = {677--695},
publisher = {IEEE},
title = {{Visual interpretation of hand gestures for human-computer interaction: A review}},
volume = {19},
year = {1997}
}
@article{Sahane,
author = {Sahane, M S and Salve, H D and Dhawade, N D and Bajpai, S A},
file = {:Users/hadeelayoub/Desktop/10.1.1.53.6402.pdf:pdf;:Users/hadeelayoub/Desktop/1-s2.0-S0262885603000702-main.pdf:pdf},
journal = {environments (VEs)},
pages = {53},
title = {{Visual Interpretation Of Hand Gestures For Human Computer Interaction}},
volume = {2}
}
@online{EnableTalkImagineCup,
title = {{Enable Talk}},
url = {https://techcrunch.com/2012/07/09/enable-talk-imagine-cup/},
urldate = {5 February 2017}
}
@article{Shinohara2009,
abstract = {Meaning can be as important as usability in the design of technology.},
author = {Shinohara, Kristen and Tenenberg, Josh},
doi = {10.1145/1536616.1536636},
isbn = {0001-0782},
issn = {0001-0782},
journal = {Commun. ACM},
number = {8},
pages = {58--66},
title = {{A blind person's interactions with technology}},
url = {http://portal.acm.org/ft{\_}gateway.cfm?id=1536636{\&}type=html{\&}coll=GUIDE{\&}dl=GUIDE{\&}CFID=52723775{\&}CFTOKEN=35016943{\%}5Cnhttp://delivery.acm.org/10.1145/1540000/1536636/p58-shinohara.html?key1=1536636{\&}key2=0399843521{\&}coll=GUIDE{\&}dl=GUIDE{\&}CFID=52723775{\&}CFTOKEN=35016},
volume = {52},
year = {2009}
}
@article{Cohene2007,
author = {Cohene, Tira and Baecker, R M and Marziali, Elsa and Mindy, Simona},
journal = {Universal Usability, John Wiley {\&} Sons},
pages = {357--387},
title = {{Memories of a life: a design case study for Alzheimer's disease}},
year = {2007}
}
@book{Strauss2008,
abstract = {The Third Edition of the bestselling Basics of Qualitative Research:Techniques and Procedures for Developing Grounded Theory continues to offer immensely practical advice and technical expertise to aid researchers in making sense of their collected data. Authors Juliet Corbin and the late Anselm Strauss (co-creator of Grounded Theory) present methods that enable researchers to analyze and interpret their data, and ultimately build theory from it. Highly accessible in their approach, Corbin and Strauss provide a step-by-step guide to the research act-from the formation of the research question through several approaches to coding and analysis, to reporting on the research. Full of definitions and illustrative examples, this book concludes with chapters that present criteria for evaluating a study, as well as responses to common questions posed by students of qualitative research. Significantly revised, Basics of Qualitative Research remains a landmark volume in the study of qualitative methods. Key Features of the Third Edition: Allows for students to develop their critical thinking skills in the "Critical Issues" section at the end of each chapter. Shows the actual steps involved in data analysis (from description to grounded theory) and data gathering by means of theoretical sampling. Provides exercises for thinking, writing and group discussion that reinforces material presented in the text. Consists of a student companion Web site at that includes real data and practice with qualitative software such as MAXQDA, as well as student practice exercises.},
author = {Strauss, Anselm and Corbin, Juliet},
booktitle = {Basics of qualitative research: Grounded theory procedures and techniques. Newbury},
doi = {10.4135/9781452230153},
isbn = {0803959400},
issn = {00222437},
pages = {379},
pmid = {11486343},
title = {{Strauss, A., {\&} Corbin, J. (1990).}},
volume = {3},
year = {2008}
}
@article{Strauss1990,
abstract = {Strauss, A., {\&} Corbin, J. (1990). Basics of qualitative research: Grounded theory procedures and techniques. Newbury Park, CA: Sage},
author = {Strauss, A and Corbin, J},
doi = {10.4135/9781452230153},
isbn = {0803932502},
issn = {00222437},
journal = {Basics of Qualitative Research},
pmid = {11486343},
title = {{Grounded theory procedures and techniques}},
url = {http://scholar.google.co.uk/scholar?q=Corbin{\%}2C+J.{\%}2C+Strauss{\%}2C+A.+{\%}281990{\%}29{\%}2C{\&}btnG={\&}hl=en{\&}as{\_}sdt=0{\%}2C5{\#}7},
year = {1990}
}
@article{Adams1997,
abstract = {To date, system research has focused on designing security mechanisms to protect systems access although their usability has rarely been investigated. This paper reports a study in which users perceptions of password mechanisms were investigated through questionnaires and interviews. Analysis of the questionnaires shows that many users report problems, linked to the number of passwords and frequency of password use. In-depth analysis of the interview data revealed that the degree to which users conform to security mechanisms depends on their perception of security levels, information sensitivity and compatibility with work practices. Security mechanisms incompatible with these perceptions may be circumvented by users and thereby undermine system security overall.},
author = {Adams, Anne and Sasse, Martina Angela and Lunt, Peter},
doi = {10.1145/99977.99993},
isbn = {3540761721},
issn = {02686139},
journal = {People and Computers},
keywords = {grounded theory,organisational factors,passwords,security},
number = {1},
pages = {1--15},
title = {{Making Passwords Secure and Usable}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.25.8977{\&}rep=rep1{\&}type=pdf},
volume = {34},
year = {1997}
}
@article{Georges2004,
abstract = {This article evaluates the notion and practice of action research, as defined by contributors to the recent special issue 'Action Research and Emancipation' in this journal. The author argues that, although action research implicitly has a design orientation, it largely draws on the humanities and sciences as its main role models. As a result, action researchers nowadays do not see themselves as design professionals. Idealized design methods serve to illustrate how design research can help realize the emancipatory intentions of action researchers.},
author = {Georges, A and Romme, L},
doi = {10.1002/casp.794},
isbn = {1099-1298},
issn = {10529284},
journal = {Journal of Community {\&} Applied Social Psychology J. Community Appl. Soc. Psychol},
keywords = {action research,design research,emancipation,ideal solution,pragmatic experimentation},
number = {June},
pages = {495--499},
title = {{Commentary Action Research, Emancipation and Design Thinking}},
volume = {14},
year = {2004}
}
@article{Glaser1967,
abstract = {Take free online classes from 80+ top universities and organizations. Coursera is a social entrepreneurship company partnering with Stanford University, Yale University, Princeton University and others around the world to offer courses online for anyone to take, for free. We believe in connecting people to a great education so that anyone around the world can learn without limits.},
author = {Glaser, B. and Strauss, A.},
isbn = {0202300285},
journal = {Weidenfield {\&} Nicolson, London},
title = {{The discovery of grounded theory. 1967}},
url = {https://www.coursera.org/},
year = {1967}
}
@inproceedings{brown2004grounded,
author = {Brown, Emily and Cairns, Paul},
booktitle = {CHI'04 extended abstracts on Human factors in computing systems},
organization = {ACM},
pages = {1297--1300},
title = {{A grounded investigation of game immersion}},
year = {2004}
}
@incollection{OBrienRoryFacultyofInformationStudies2001,
author = {{O'Brien, Rory (Faculty of Information Studies}, University of Toronto)},
title = {{An Overview of the Methodological Approach of Action Research}},
url = {http://www.web.ca/{~}robrien/papers/arfinal.html},
year = {2001}
}
@article{Wang2005,
abstract = {During the past decade, design-based research has demonstrated its potential as a methodology suitable to both research and design of technology-enhanced learning environments (TELEs). In this paper, we define and identify characteristics of design-based research, describe the importance of design-based research for the development of TELEs, propose principles for implementing design-based research with TELEs, and discuss future challenges of using this methodology},
author = {Wang, Feng and Hannafin, Michael J.},
doi = {10.2307/30221206},
isbn = {1042-1629},
issn = {10421629},
journal = {Educational Technology Research and Development},
number = {4},
pages = {5--23},
title = {{Design-Based Reserach and Technology-Enhanced Learning environments}},
volume = {53},
year = {2005}
}
@article{ThomasGilmore1986,
author = {Gilmore, Thomas and Krantz, Jim and Ramirez, Rafael},
title = {{Action Based Modes of Inquiry and the Host-Researcher Relationship}},
year = {1986}
}
@online{5DTech,
title = {{5th Dimention Technologies}},
url = {http://www.5dt.com/wp-content/uploads/2011/06/hw{\_}data{\_}glove{\_}wireless{\_}01.jpg},
urldate = {2017-01-27}
}
@online{FingerSpelling,
title = {{Fingerspelling}},
url = {https://en.wikipedia.org/wiki/Fingerspelling},
urldate = {2017-01-27}
}
@techreport{XISTDataGlove,
file = {:Users/hadeelayoub/Desktop/X-IST{\_}DataGlove{\_}manual{\_}V1.17.pdf:pdf},
title = {{X-IST Data Glove}},
url = {http://www.lon3d.com/Soft/UploadSoft/201103/X-IST{\_}DataGlove{\_}manual{\_}V1.17.pdf}
}
@online{CyberGloveII,
title = {{CyberGlove II}},
url = {http://www.cyberglovesystems.com/cyberglove-ii},
urldate = {2017-01-27}
}
@online{AcceleGlove2017,
title = {{AcceleGlove Dr.Jose}},
url = {http://www.mobilemag.com/2003/08/05/inventor-develops-acceleglove-talking-glove-for-deaf/},
urldate = {2017-01-27}
}
@online{P5Glove,
title = {{P5 Glove}},
url = {http://www.mindflux.com.au/products/essentialreality/p5glove.html},
urldate = {2017-01-27}
}
@inproceedings{DipietroL.SabatiniA.M.Dario2008,
author = {Dipietro, L. and Sabatini, A.M. and Dario, P.},
booktitle = {IEEE Trans. Syst. Man Cybern.},
pages = {461--482},
title = {{Survey of glove-based systems and their applications}},
year = {2008}
}
@online{Rozak,
author = {Rozak, R.},
title = {{Sign Language Translator}},
url = {http://www.romanakozak.com/sign-language-translator/]},
urldate = {2017-01-25}
}
@online{MexicoPoly,
title = {{Mexico's National Polytechnic Institute}},
url = {http://gadgets.ndtv.com/wearables/news/new-smart-glove-can-translate-sign-language-712973},
urldate = {2017-01-01}
}
@online{EnableTalk,
title = {{EnableTalk}},
url = {http://enabletalk.com},
urldate = {2017-01-01}
}
@inproceedings{FreundY.Schapire1995,
author = {Freund, Y. and Schapire, R.E.},
booktitle = {European Conference on Computational Learning Theory},
pages = {23--37},
title = {{A decision-theoretic generalization of on-line learning and an application to boosting}},
year = {1995}
}
@book{Stokoe1960,
author = {Stokoe, W.C.},
pages = {3--37},
title = {{Sign language structure: An outline of the visual communication systems of the american deaf}},
year = {1960}
}
@inproceedings{GastaldiG.PareschiA.SabatiniS.SolariF.Bisio2005,
author = {Gastaldi, G. and Pareschi, A. and Sabatini, S. and Solari, F. and Bisio, G.M.},
booktitle = {IEEE International Confer- ence on Image Processing},
pages = {397--400},
title = {{A Man Machine Communication System based on the Visual Analysis of Dynamic Gestures}},
year = {2005}
}
@online{AcceleGlove2016,
title = {{AcceleGlove}},
url = {http://www.wtol.com/story/1387619/sign-language-glove-helps-deaf-communicate},
urldate = {2016-10-15}
}
@online{CornellGlove,
title = {{Cornell University Glove}},
url = {http://people.ece.cornell.edu/land/courses/ece4760/FinalProjects/f2014/rdv28{\_}mjl256/webpage/}
}
@inproceedings{WaldronM.B.Simon1989,
author = {Waldron, M.B. and Simon, D.},
booktitle = {Annual International Conference of the IEEE Engineering in Engineering in Medicine and Biology Society},
pages = {1798--1799},
title = {{Parsing method for signed telecommunication}},
year = {1989}
}
@article{Liddell1989,
abstract = {Outlines phonological structure and processes of American Sign Language (ASL), covering: (1) sequential phenomena found in the production of individual signs; (2) the segmental phonetic transcription system; (3) paradigmatic and syntagmatic contrasts in ASL signs; (4) similarities between ASL and spoken language phonological processes; and (5) phonological effects on morphological processes. (20 references) (Author/CB)},
author = {Liddell, Scott K. and Johnson, Robert E.},
doi = {10.1353/sls.1989.0027},
issn = {0302-1475, 0302-1475},
journal = {Sign Language Studies},
pages = {195--278},
title = {{American Sign Language: The phonological base}},
volume = {64},
year = {1989}
}
@article{Zhang2009,
abstract = {This paper describes a novel hand gesture recognition system that utilizes both multi-channel surface electromyogram (EMG) sensors and 3D accelerometer (ACC) to realize user-friendly interaction between human and computers. Signal segments of meaningful gestures are determined from the continuous EMG signal inputs. Multi-stream Hidden Markov Models consisting of EMG and ACC streams are utilized as decision fusion method to recognize hand gestures. This paper also presents a virtual Rubik's Cube game that is controlled by the hand gestures and is used for evaluating the performance of our hand gesture recognition system. For a set of 18 kinds of gestures, each trained with 10 repetitions, the average recognition accuracy was about 91.7{\%} in real application. The proposed method facilitates intelligent and natural control based on gesture interaction.},
author = {Zhang, Xu and Chen, Xiang and Wang, W and Yang, Ji-hai},
doi = {10.1145/1502650.1502708},
isbn = {9781605583310},
journal = {Proceedings of the 14th {\ldots}},
number = {September 2015},
pages = {401},
title = {{Hand gesture recognition and virtual game control based on 3D accelerometer and EMG sensors}},
url = {http://portal.acm.org/citation.cfm?doid=1502650.1502708{\%}5Cnhttp://dl.acm.org/citation.cfm?id=1502708},
year = {2009}
}
@article{AnethaK2014,
abstract = {Everyday communication with the hearing population poses a major challenge to those with hearing loss. For this purpose, an automatic American Sign Language recognition system is developed using artificial neural network (ANN) and to translate the ASL alphabets into text and sound. A glove circuit is designed with flex sensors, 3- axis accelerometer and sEMG sensors to capture the gestures. The finger bending data is obtained from the flex sensors on each finger whereas the accelerometer provides the trajectories of the hand motion. Some local features are extracted from the ASL alphabets which are then classified using neural network. The proposed system is evaluated for both user-dependent and user-independent conditions successfully for isolated ASL recognition. The main purpose of this Hand Talk system is to provide an ease of sharing ideas, minimized communication gap and an easier collaboration for the hard of hearing people.},
archivePrefix = {arXiv},
arxivId = {ISSN(Online):2320-9801},
author = {{Anetha K}, Rejina Parvin J.},
eprint = {ISSN(Online):2320-9801},
file = {:Users/hadeelayoub/Library/Application Support/Mendeley Desktop/Downloaded/Anetha K - 2014 - Hand Talk-A Sign Language Recognition Based On Accelerometer and SEMG Data.pdf:pdf},
journal = {International Journal of Innovative Research in Computer and Communication Engineering},
keywords = {Accelerometer,Artificial Neural Network,Electromyography,Flex Sensors,Sign Language Recognition},
number = {3},
pages = {206--215},
title = {{Hand Talk-A Sign Language Recognition Based On Accelerometer and SEMG Data}},
url = {www.ijircce.com},
volume = {2},
year = {2014}
}
@book{Ong2005,
abstract = {Research in automatic analysis of sign language has largely focused on recognizing the lexical (or citation) form of sign gestures as they appear in continuous signing, and developing algorithms that scale well to large vocabularies. However, successful recognition of lexical signs is not sufficient for a full understanding of sign language communication. Nonmanual signals and grammatical processes which result in systematic variations in sign appearance are integral aspects of this communication but have received comparatively little attention in the literature. In this survey, we examine data acquisition, feature extraction and classification methods employed for the analysis of sign language gestures. These are discussed with respect to issues such as modeling transitions between signs in continuous signing, modeling inflectional processes, signer independence, and adaptation.We further examine works that attempt to analyze nonmanual signals and discuss issues related to integrating these with (hand) sign gestures.Wealso discuss the overall progress toward a true test of sign recognition systems—dealing with natural signing by native signers. We suggest some future directions for this research and also point to contributions it can make to other fields of research. Web-based supplemental materials (appendicies) which contain several illustrative examples and videos of signing can be found at www.computer.org/publications/dlib.},
author = {Ong, Sylvie C W and Ranganath, Surendra},
booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
doi = {10.1109/TPAMI.2005.112},
isbn = {01628828 (ISSN)},
issn = {01628828},
keywords = {Face tracking,Facial expression recognition,Gesture analysis,Hand gesture recognition,Hand tracking,Head gesture recognition,Head tracking,Review,Sign language recognition},
number = {6},
pages = {873--891},
pmid = {15943420},
title = {{Automatic sign language analysis: A survey and the future beyond lexical meaning}},
volume = {27},
year = {2005}
}
@article{Starner1998,
abstract = {We present two real-time hidden Markov model-based systems for recognizing sentence-level continuous American sign language (ASL) using a single camera to track the user's unadorned hands. The first system observes the user from a desk mounted camera and achieves 92 percent word accuracy. The second system mounts the camera in a cap worn by the user and achieves 98 percent accuracy (97 percent with an unrestricted grammar). Both experiments use a 40-word lexicon},
author = {Starner, T. and Weaver, J. and Pentland, A.},
doi = {10.1109/34.735811},
isbn = {0162-8828},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {12},
pages = {1371--1375},
title = {{Real-time American sign language recognition using desk and wearable computer based video}},
volume = {20},
year = {1998}
}
@article{Mantyjarvi2004,
abstract = {Accelerometer based gesture control is proposed as a complementary interaction modality for handheld devices. Predetermined gesture commands or freely trainable by the user can be used for controlling functions also in other devices. To support versatility of gesture commands in various types of personal device applications gestures should be customisable, easy and quick to train. In this paper we experiment with a procedure for training/recognizing customised accelerometer based gestures with minimum amount of user effort in training. Discrete Hidden Markov Models (HMM) are applied. Recognition results are presented for an external device, a DVD player gesture commands. A procedure based on adding noise-distorted signal duplicates to training set is applied and it is shown to increase the recognition accuracy while decreasing user effort in training. For a set of eight gestures, each trained with two original gestures and with two Gaussian noise- distorted duplicates, the average recognition accuracy was 97{\%}, and with two original gestures and with four noise- distorted duplicates, the average recognition accuracy was 98{\%}, cross-validated from a total data set of 240 gestures. Use of procedure facilitates quick and effortless customisation in accelerometer based interaction.},
author = {M{\"{a}}ntyj{\"{a}}rvi, Jani and Kela, Juha and Korpip{\"{a}}{\"{a}}, Panu and Kallio, Sanna},
doi = {http://doi.acm.org/10.1145/1052380.1052385},
isbn = {1-58113-981-0},
journal = {Proceedings of the 3rd International Conference on Mobile and Ubiquitous Multimedia MUM 04},
keywords = {Gesture recognition,Human computer interaction,gesture control,input technology,mobile devices},
pages = {25--31},
title = {{Enabling fast and effortless customisation in accelerometer based gesture interaction}},
url = {http://dl.acm.org/citation.cfm?id=1052385},
year = {2004}
}
@book{Lazar2010,
abstract = {Continual technological evolution has led to an explosion of new techniques in Human-Computer Interaction (HCI) research. Research Methods in Human-Computer Interaction is a thoroughly comprehensive guide to performing research and is essential reading for both quantitative and qualitative methods. Chapters cover a broad range of topics relevant to the collection and analysis of HCI data, going beyond experimental design and surveys, to cover ethnography, time diaries, physiological measurements, case studies, and other essential elements in the well-informed HCI researcher's toolkit. This book is a must read for anyone in the field of Human-Computer Interaction. The multi-disciplinarian approach, housed in the reality of the technological world today, makes for a practical and informative guide for user interface designers, software and hardware engineers and anyone doing user research.Dr. Mary Czerwinski, Research AreaManager, Microsoft Research, USA Research Methods in HCI is an excellent read for practitioners and students alike. It discusses all the must-know theory, provides detailed instructions on how to carry out the research, and offers great examples. I loved it! Professor Vanessa Evers, Professor, Human Computer Studies Lab, University of Amsterdam, the Netherlands "The book is superb: comprehensive, clear, and engaging! This is a one-stop HCI methods reference library. If you can only buy one HCI methods book, this is the one!"Dr. Clare-Marie Karat, IBM TJ Watson Research, USA, and recipient of the 2009 ACM SIGCHI Lifetime Service Award A much needed and very useful book, covering important HCI research methods overlooked in standard research methods texts. Professor Gilbert Cockton, School of Design, Northumbria University, United Kingdom},
author = {Lazar, Jonathan and Feng, Jinjuan Heidi and Hochheiser, Harry},
booktitle = {Research Methods in HumanComputer Interaction},
isbn = {0470723378, 9780470723371},
pages = {426},
title = {{Research Methods in Human-Computer Interaction}},
url = {http://dl.acm.org/citation.cfm?id=1841406{\%}5Cnhttp://www.amazon.com/Research-Methods-Human-Computer-Interaction-Jonathan/dp/0470723378},
year = {2010}
}
@book{Cox2008,
abstract = {The aim of the tutorial is to help researchers, particularly early career researchers, to develop the appropriate skills to make a useful research contribution to Human-Computer Interaction (HCI). This is in recognition of the fact that HCI draws on a wide variety of disciplines which means that there is a wide variety of methods that a researcher could use and moreover new researchers may have education or experience in only a small fraction of the methods available.},
author = {Cox, Anna L and Cairns, Paul},
booktitle = {Proceedings of the 22nd British HCI Group Annual Conference on People and Computers: Culture, Creativity, Interaction - Volume 2 (BCS-HCI '08), Vol. 2},
doi = {10.1017/CBO9780511814570},
isbn = {978-0-521-69031-7},
issn = {15322882},
keywords = {analysis,and focus groups,cognitive modelling,controlled experiments,eyetracking,formal,in-depth interviews,methodological,qualitative analysis,questionnaires,statistics},
pages = {221--222},
title = {{Research Methods for Human-Computer Interaction}},
url = {http://dl.acm.org/citation.cfm?id=1457554},
year = {2008}
}
@article{Viola2001,
abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "integral image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Viola, P and Jones, M},
doi = {10.1109/CVPR.2001.990517},
eprint = {arXiv:1011.1669v3},
isbn = {0-7695-1272-0},
issn = {1063-6919},
journal = {Computer Vision and Pattern Recognition (CVPR)},
keywords = {AdaBoost,Detectors,Face detection,Filters,Focusing,Image representation,Machine learning,Object detection,Pixel,Robustness,Skin,background regions,boosted simple feature cascade,classifiers,face detection,feature extraction,image classification,image processing,image representation,integral image,learning (artificial intelligence),machine learning,object detection,object specific focus-of-attention mechanism,rapid object detection,real-time applications,statistical guarantees,visual object detection},
pages = {I----511----I----518},
pmid = {7143246},
title = {{Rapid object detection using a boosted cascade of simple features}},
volume = {1},
year = {2001}
}
@article{Dix2004,
abstract = {Much has changed since the first EDITION of humancomputer interaction was published. Ubiquitous computing and rich sensor-filled environments are finding their way out of the laboratory, not just into movies but also into our workplaces and homes. The computer has broken out of its plastic and glass bounds providing us with networked societies where personal computing devices from mobile phones to smart cards fill our pockets and electronic devices surround us at home and work. The web too has grown from a largely academic network into the hub of business and everyday lives. As the distinctions between the physical and the digital, and between work and leisure start to break down, human-computer interaction is also changing radically. The excitement of these changes is captured in this new EDITION, which also looks forward to other emerging technologies. However, the book is firmly rooted in strong principles and models indepENDent of the passing technologies of the day: these foundations will be the means by which today's students will understand tomorrow's technology. The third EDITION of humancomputer interaction can be used for introductory and advanced courses on HCI, Interaction Design, Usability or Interactive Systems Design. It will also prove an invaluable reference for professionals wishing to design usable computing devices. Accompanying the text is a comprehensive website containing a broad range of material for instructors, students and practitioners, a full text search facility for the book, links to many sites of additional interest and much more: go to www.hcibook.com New to this EDITION: bull; bull;A revised structure, reflecting the growth of HCI as a discipline, separates out basic material suitable for introductory courses from more detailed models and theories. bull;New chapter on Interaction Design adds material on scenarios and basic navigation design. bull;New chapter on Universal Design, substantially extENDing the coverage of this material in the book. bull;Updated and extENDed treatment of socio/contextual issues. bull;ExtENDed and new material on novel interaction, including updated ubicomp material, designing experience, physical sensors and a new chapter on rich interaction. bull;Updated material on the web including dynamic content and WAP. Alan Dix is Professor in the Department of Computing, Lancaster, UK. Janet Finlay is Professor at the School of Computing, Leeds Metropolitan University, UK. Gregory Abowd is Assistant Professor in the College of Computing at Georgia Tech, USA. Russell Beale is lecturer at the School of Computer Science, University of Birmingham, UK.},
author = {Dix, A and Finlay, J and Abowd, G D and Beale, R},
doi = {10.1207/S15327051HCI16234},
isbn = {0130461091},
issn = {01304610},
journal = {Human-Computer Interaction},
number = {January},
pages = {834},
pmid = {20978922},
title = {{Human-Computer Interaction}},
url = {http://www.amazon.com/Human-Computer-Interaction-3rd-Alan-Dix/dp/0130461091},
volume = {Third},
year = {2004}
}
@inproceedings{Parvini2009,
abstract = {Nowadays, computer interaction is mostly done using dedicated devices. But gestures are an easy mean of expression between humans that could be used to communicate with computers in a more natural manner. Most of the current research on hand gesture recognition for Human-Computer Interaction rely on either the Neural Networks or Hidden Markov Models (HMMs). In this paper, we compare different approaches for gesture recognition and highlight the major advantages of each. We show that gestures recognition based on the Bio-mechanical characteristic of the hand provides an intuitive approach which provides more accuracy and less complexity. {\textcopyright} 2009 Springer Berlin Heidelberg.},
author = {Parvini, Farid and McLeod, Dennis and Shahabi, Cyrus and Navai, Bahareh and Zali, Baharak and Ghandeharizadeh, Shahram},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-02577-8_26},
isbn = {3642025765},
issn = {03029743},
number = {PART 2},
pages = {236--245},
title = {{An approach to glove-based gesture recognition}},
volume = {5611 LNCS},
year = {2009}
}
@article{Binh2005,
abstract = {In this paper, we introduce a hand gesture recognition system to recognize real time gesture in unconstrained environments. The system consists of three modules: real time hand tracking, training gesture and gesture recognition using pseudo two dimension hidden Markov models (P2-DHMMs). We have used a Kalman filter and hand blobs analysis for hand tracking to obtain motion descriptors and hand region. It is fairy robust to background cluster and uses skin color for hand gesture tracking and recognition. Furthermore, there have been proposed to improve the overall performance of the approach: (1) Intelligent selection of training images and (2) Adaptive threshold gesture to remove non-gesture pattern that helps to qualify an input pattern as a gesture. A gesture recognition system which can reliably recognize single-hand gestures in real time on standard hardware is developed. In the experiments, we have tested our system to vocabulary of 36 gestures including the America sign language (ASL) letter spelling alphabet and digits, and results effectiveness of the approach. Keywords: Hand gesture recognition; Hand tracking; Kalman filter; Pseudo 2-D Hidden Markov models.},
author = {Binh, Nguyen Dang and Shuichi, Enokida and Ejima, Toshiaki},
doi = {10.1016/S0262-8856(03)00070-2},
isbn = {9781479962723},
issn = {02628856},
journal = {Proc. GVIP},
keywords = {hand gesture recognition,hand tracking,kalman filter,models,pseudo 2-d hidden markov},
number = {December},
pages = {19--21},
title = {{Real-time hand tracking and gesture recognition system}},
url = {http://imtop.googlecode.com/svn/trunk/MasterThesis{\_}SVN/Related work/Tracking/Real-Time Hand Tracking and Gesture Recognition System.pdf},
year = {2005}
}
@article{Hu1962,
abstract = {In this paper a theory of two-dimensional moment invariants for planar geometric figures is presented. A fundamental theorem is established to relate such moment invariants to the well-known algebraic invariants. Complete systems of moment invariants under translation, similitude and orthogonal transformations are derived. Some moment invariants under general two-dimensional linear transformations are also included. Both theoretical formulation and practical models of visual pattern recognition based upon these moment invariants are discussed. A simple simulation program together with its performance are also presented. It is shown that recognition of geometrical patterns and alphabetical characters independently of position, size and orientation can be accomplished. It is also indicated that generalization is possible to include invariance with parallel projection.},
author = {Hu, Ming-Kuei},
doi = {10.1109/TIT.1962.1057692},
isbn = {0096-1000},
issn = {0096-1000},
journal = {IRE Transactions on Information Theory},
keywords = {Artificial intelligence,Bibliographies,Character recognition,Decision theory,Distribution functions,Image analysis,Information processing,Information theory,Pattern recognition,Senior members,Shape},
pages = {179--187},
title = {{Visual pattern recognition by moment invariants}},
volume = {8},
year = {1962}
}
@article{Premaratne2007,
abstract = {Almost all consumer electronic equipment today uses remote controls for user interfaces. However, the variety of physical shapes and functional commands that each remote control features also raises numerous problems: the difficulties in locating the required remote control, the confusion with the button layout, the replacement issue and so on. The consumer electronics control system using hand gestures is a new innovative user interface that resolves the complications of using numerous remote controls for domestic appliances. Based on one unified set of hand gestures, this system interprets the user hand gestures into pre-defined commands to control one or many devices simultaneously. The system has been tested and verified under both incandescent and fluorescent lighting conditions. The experimental results are very encouraging as the system produces real-time responses and highly accurate recognition towards various gestures},
author = {Premaratne, P. and Nguyen, Q.},
doi = {10.1049/iet-cvi:20060198},
issn = {17519632},
journal = {IET Computer Vision},
number = {1},
pages = {35},
title = {{Consumer electronics control system based on hand gesture moment invariants}},
url = {http://digital-library.theiet.org/content/journals/10.1049/iet-cvi{\_}20060198},
volume = {1},
year = {2007}
}
@article{Gennery1992,
abstract = {A method is described of visually tracking a known three-dimensional object as it moves with six degrees of freedom. The method uses the predicted position of known features on the object to find the features in images from one or more cameras, measures the position of the features in the images, and uses these measurements to update the estimates of position, orientation, linear velocity, and angular velocity of the object model. The features usually used are brightness edges that correspond to markings or the edges of solid objects, although point features can be used. The solution for object position and orientation is a weighted least-squares adjustment that includes filtering over time, which reduces the effects of errors, allows extrapolation over times of missing data, and allows the use of stereo information from multiple-camera images that are not coincident in time. The filtering action is derived so as to be optimum if the acceleration is random. (Alternatively, random torque can be assumed for rotation.) The filter is equivalent to a Kalman filter, but for efficiency it is formulated differently in order to take advantage of the dimensionality of the observations and the state vector which occur in this problem. The method can track accurately with arbitrarily large angular velocities, as long as the angular acceleration (or torque) is small. Results are presented showing the successful tracking of partially obscured objects with clutter.},
author = {Gennery, Donald B.},
doi = {10.1007/BF00126395},
isbn = {0920-5691},
issn = {09205691},
journal = {International Journal of Computer Vision},
number = {3},
pages = {243--270},
title = {{Visual tracking of known three-dimensional objects}},
volume = {7},
year = {1992}
}
@inproceedings{Premaratne2011,
abstract = {Dynamic hand gesture tracking and recognition system can simplify the way humans interact with computers and many other non-critical consumer electronic equipments. This system is based on the well-known "Wave Controller" technology developed at the University of Wollongong [1-3] and certainly a step forward in video gaming and consumer electronics control interfaces. Many computer interfaces used today such as keyboard, mouse, joystick or gaming wheels have constrained the artistic ability of many users, as they are required to respond to the computer through pressing buttons or moving other apparatus. Most of the drawbacks of the modern interfaces can be tackled by using a reliable hand gesture tracking and recognition system based on both Lucas-Kanade and Moment Invariants approaches. The realtime functional ability of this system will enhance the user experience as users are no longer have any physical connection to the equipment being controlled. {\textcopyright} 2012 Springer-Verlag.},
author = {Premaratne, Prashan and Ajaz, Sabooh and Premaratne, Malin},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-25944-9_76},
isbn = {9783642259432},
issn = {03029743},
keywords = {Computer Human Interaction (HCI),Dynamic,Gesture,Lucas-Kanade Algorithm,Moment Invariants,Recognition,Support Vector Machines,Tracking},
pages = {588--593},
title = {{Hand gesture tracking and recognition system for control of consumer electronics}},
volume = {6839 LNAI},
year = {2011}
}
@article{Segen1998,
abstract = {Note: OCR errors may be found in this Reference List extracted from the full text article. ACM has opted to expose the complete List rather than only correct and linked references.},
author = {Segen, Jaub and Kumar, Sentm},
doi = {10.1145/290747.290822},
isbn = {0201309904},
journal = {Proceedings of the sixth ACM international {\ldots}},
pages = {455--464},
title = {{Gesture vr: vision-based 3d hand interace for spatial interaction}},
url = {http://dl.acm.org/citation.cfm?id=290822},
year = {1998}
}
@article{Lamberti2012,
abstract = {This paper presents Handy, a real-time hand gesture recognizer based on a three color glove. The recognizer is formed by three modules. The first module, fed by the frame acquired by a webcam, identifies the hand image in the scene. The second module, a feature extractor, represents the image by a nine-dimensional feature vector. The third module, the classifier, is performed by means of learning vector quantization. The recognizer, tested on a dataset of 907 hand gestures, has shown very high recognition rate. ?? 2012 Elsevier Ltd. All rights reserved.},
author = {Lamberti, Luigi and Camastra, Francesco},
doi = {10.1016/j.eswa.2012.02.081},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Color glove,Data glove,Gesture recognition,HSI color space,Learning vector quantization,Real time},
number = {12},
pages = {10489--10494},
title = {{Handy: A real-time three color glove-based gesture recognizer with learning vector quantization}},
volume = {39},
year = {2012}
}
@article{Han2009,
abstract = {Modelling and segmenting subunits is one of the important topics in sign language study. Many scholars have proposed the functional definition to subunits from the view of linguistics while the problem of efficiently implementing it using computer vision techniques is a challenge. On the other hand, a number of subunit segmentation work has been investigated for the task of vision-based sign language recognition whereas their subunits either somewhat lack the linguistic support or are improper. In this paper, we attempt to define and segment subunits using computer vision techniques, which also can be basically explained by sign language linguistics. A subunit is firstly defined as one continuous visual hand action in time and space, which comprises a series of interrelated consecutive frames. Then, a simple but efficient solution is developed to detect the subunit boundary using hand motion discontinuity. Finally, temporal clustering by dynamic time warping is adopted to merge similar segments and refine the results. The presented work does not need prior knowledge of the types of signs or number of subunits and is more robust to signer behaviour variation. Furthermore, it correlates highly with the definition of syllables in sign language while sharing characteristics of syllables in spoken languages. A set of comprehensive experiments on real-world signing videos demonstrates the effectiveness of the proposed model. ?? 2009 Elsevier B.V. All rights reserved.},
author = {Han, Junwei and Awad, George and Sutherland, Alistair},
doi = {10.1016/j.patrec.2008.12.010},
isbn = {01678655 (ISSN)},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Dynamic time warping,Hand motion,Phoneme,Sign language recognition,Subunit},
number = {6},
pages = {623--633},
title = {{Modelling and segmenting subunits for sign language recognition based on hand motion analysis}},
volume = {30},
year = {2009}
}
@article{Starner1995,
abstract = {Submitted to the Program in Media Arts and Sciences, School of Architecture and Planning, in partial ful llment of the requirements for the degree of MASTER OF SCIENCE IN MEDIA ARTS AND SCIENCES at the Massachusetts Institute of Technology February 1995},
author = {Starner, Thad Eugene and Benton, Stephen a},
journal = {International Workshop on Automatic Face and Gesture Recognition},
pages = {189--194},
title = {{Visual Recognition of American Sign Language Using Hidden Markov Models}},
year = {1995}
}
@book{Nickel2004,
abstract = {We present our approach for visual tracking of head, hands and head orientation. Given the images provided by a calibrated stereo-camera, color and disparity information are integrated into a multi-hypotheses tracking framework in order to find the 3D-positions of the respective body parts. Based on the hands' motion, an HMM-based approach is applied to recognize pointing gestures. We show experimentally, that the gesture recognition performance can be improved significantly by using visually gained information about head orientation as an additional feature. Our system aims at applications in the field of human-robot interaction, where it is important to do run-on recognition in real-time, to allow for robot's egomotion and not to rely on manual initialization.},
author = {Nickel, K and Scemann, E and Stiefelhagen, R},
booktitle = {Automatic Face and Gesture Recognition, 2004. Proceedings. Sixth IEEE International Conference on},
doi = {10.1109/AFGR.2004.1301593},
isbn = {VO -},
keywords = {3D-tracking,Biological system modeling,Face detection,Face recognition,HMM-based approach,Hidden Markov models,Human robot interaction,Interactive systems,Magnetic heads,Nickel,Real time systems,Speech recognition,calibrated stereo-camera,disparity information,gesture recognition,head orientation,hidden Markov models,human-robot interaction,human-robot interaction scenario,man-machine systems,multi-hypotheses tracking framework,pointing gesture recognition,robot egomotion,run-on recognition,stereo image processing,tracking,visual tracking},
pages = {565--570},
title = {{3D-tracking of head and hands for pointing gesture recognition in a human-robot interaction scenario}},
year = {2004}
}
@article{Berci2007,
abstract = {There is a great need for alternative human-machine interfaces in several application areas. In the current paper we present the first stage of a hand gesture recognition system with the primary purpose of replacing classical input peripherals like the mouse and the keyboard. The system works by visual input and the processing has been implemented on the Cellular Neural/Nonlinear Network paradigm based Bi-i visual processing architecture. Its properties, most notably the real-time performance, allows it to be used for security, military, medical, surgery and public media applications as well.},
author = {Berci, N and Szolgay, P},
journal = {2007 EUROPEAN CONFERENCE ON CIRCUIT THEORY AND DESIGN, VOLS 1-3;: 496-499 2007},
pages = {496--499},
title = {{Vision based human-machine interface via hand gestures}},
url = {https://www.thomsoninnovation.com/tip-innovation/{\%}5Cnhttps://www.thomsoninnovation.com/tip-innovation/recordView.do?datasource=WOK{\&}category=LIT{\&}selRecord=1{\&}totalRecords=1{\&}databaseIds=WOS{\&}idType=uid/recordid{\&}recordKeys=000258708400121/WOS:000258708400121},
year = {2007}
}
@inproceedings{Zou2010,
abstract = {We have developed a dynamic hand gesture recognition system that can simplify the way humans interact with computers and many other non-critical consumer electronic equipment. The proposed system is based on the well-known "Wave Controller" technology developed at the University of Wollongong [1-3] and will revolutionize video gaming and consumer electronics control interfaces. Currently, computer interfacing mainly involves keyboard, mouse, joystick or gaming wheels and occasionally voice recognition for user input. These modes of interaction have restrained the artistic ability of many users, as they are required to respond to the computer through pressing buttons or moving other apparatus. Voice recognition is seen as unreliable and impractical in areas where more than one user is present. All these drawbacks can be tackled by using a reliable hand gesture recognition system that facilitates interaction between users and computers and other consumer electronic equipment in real time. This will further enhance the user experience as users no longer have any physical connection to the equipment being controlled. This system can also be extended to a sign language system for the benefit of the disabled including those with speech disabilities. {\textcopyright} 2010 IEEE.},
author = {Zou, Z. and Premaratne, P. and Monaragala, R. and Bandara, N. and Premaratne, M.},
booktitle = {Proceedings of the 2010 5th International Conference on Information and Automation for Sustainability, ICIAfS 2010},
doi = {10.1109/ICIAFS.2010.5715644},
isbn = {9781424485512},
title = {{Dynamic hand gesture recognition system using moment invariants}},
year = {2010}
}
@inproceedings{Iwai1996,
abstract = {This paper describes a method to recognize hand gestures from a monocular image in real time. We use colored gloves to detect specific region of hand quickly. By using this method, we can treat easily the occlusion problem due to color information. Hand gesture are quickly recognized by the decision tree made from the image features automatically by our proposed method. There are another pattern recognition methods using image features like nearest-neighbor method. But it is better for computation time to use the decision tree because the decision tree uses only few selected important features. In this paper, we show a learning result by the CAD model made automatically and recognition results in real images by our proposed method.},
author = {Iwai, Yoshio and Watanabe, Ken and Yagi, Yasushi and Yachida, Masahiko},
booktitle = {Proceedings - International Conference on Pattern Recognition},
doi = {10.1109/ICPR.1996.546107},
isbn = {081867282X},
issn = {10514651},
pages = {662--666},
title = {{Gesture recognition using colored gloves}},
volume = {1},
year = {1996}
}
@inproceedings{Kong2008,
abstract = {This paper presents an automatic approach to segment 3-D hand trajectories and transcribe phonemes based on them, as a step towards recognizing American sign language (ASL).We first apply a segmentation algorithm which detects minimal velocity and maximal change of directional angle to segment the hand motion trajectory of naturally signed sentences. This yields over-segmented trajectories, which are further processed by a trained naive Bayesian detector to identify true segmented points and eliminate false alarms. The above segmentation algorithm yielded 88.5{\%} true segmented points and 11.8{\%} false alarms on unseen ASL sentence samples. These segmentation results were refined by a simple majority voting scheme, and the final segments obtained were used to transcribe phonemes for ASL. This was based on clustering PCA-based features extracted from training sentences. We then trained hidden Markov models (HMMs) to recognize the sequence of phonemes in the sentences. On the 25 test sentences containing 157 segments, the average number of errors obtained was 15.6.},
author = {Kong, W. W. and Ranganath, Surendra},
booktitle = {2008 8th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2008},
doi = {10.1109/AFGR.2008.4813462},
isbn = {9781424421541},
title = {{Automatic hand trajectory segmentation and phoneme transcription for sign language}},
year = {2008}
}
@article{Chen2003,
abstract = {In this paper, we introduce a hand gesture recognition system to recognize continuous gesture before stationary background. The system consists of four modules: a real time hand tracking and extraction, feature extraction, hidden Markov model (HMM) training, and gesture recognition. First, we apply a real-time hand tracking and extraction algorithm to trace the moving hand and extract the hand region, then we use the Fourier descriptor (FD) to characterize spatial features and the motion analysis to characterize the temporal features. We combine the spatial and temporal features of the input image sequence as our feature vector. After having extracted the feature vectors, we apply HMMs to recognize the input gesture. The gesture to be recognized is separately scored against different HMMs. The model with the highest score indicates the corresponding gesture. In the experiments, we have tested our system to recognize 20 different gestures, and the recognizing rate is above 90{\%}.},
author = {Chen, F},
doi = {10.1016/S0262-8856(03)00070-2},
file = {:Users/hadeelayoub/Desktop/1-s2.0-S0262885603000702-main.pdf:pdf},
isbn = {02628856 (ISSN)},
issn = {02628856},
journal = {Image and Vision Computing},
keywords = {hand gesture recognition,hand tracking,hidden markov model},
number = {8},
pages = {745--758},
title = {{Hand gesture recognition using a real-time tracking method and hidden Markov models}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0262885603000702},
volume = {21},
year = {2003}
}
@inproceedings{Rehg1994,
abstract = {Computer sensing of hand and limb motion is an important problem for applications in human-computer interaction (HCI), virtual reality, and athletic performance measurement. Commercially available sensors are invasive, and require the user to wear gloves or targets. We have developed a noninvasive vision-based hand tracking system, called DigitEyes. Employing a kinematic hand model, the DigitEyes system has demonstrated tracking performance at speeds of up to 10 Hz, using line and point features extracted from gray scale images of unadorned, unmarked hands. We describe an application of our sensor to a 3D mouse user-interface problem},
author = {Rehg, J M and Kanade, T},
booktitle = {Proceedings of 1994 IEEE Workshop on Motion of Nonrigid and Articulated Objects},
doi = {10.1109/MNRAO.1994.346260},
isbn = {0-8186-6435-5},
number = {November},
pages = {16--22},
title = {{DigitEyes: vision-based hand tracking for human-computer interaction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=346260},
year = {1994}
}
@article{Kadir2004,
abstract = {This paper presents a flexible monocular system capable of recognising sign lexicons far greater in number than previous approaches. The power of the system is due to four key elements: (i) Head and hand detection based upon boosting which removes the need for temperamental colour segmentation; (ii) A body centred description of activity which overcomes issues with camera placement, calibration and user; (iii) A two stage classification in which stage I generates a high level linguistic description of activity which naturally generalises and hence reduces training; (iv) A stage II classifier bank which does not require HMMs, further reducing training requirements. The outcome of which is a system capable of running in real-time, and generating extremely high recognition rates for large lexicons with as little as a single training instance per sign. We demonstrate classification rates as high as 92{\%} for a lexicon of 164 words with extremely low training requirements outperforming previous approaches where thousands of training examples are required.},
author = {Kadir, T. and Bowden, R. and Ong, Ej and Zisserman, a.},
doi = {10.5244/C.18.96},
isbn = {1-901725-25-1},
journal = {British Machine Vision Conference},
pages = {96.1--96.10},
title = {{Minimal Training, Large Lexicon, Unconstrained Sign Language Recognition.}},
url = {http://www.bmva.org/bmvc/2004/papers/paper{\_}265.html{\%}5Cnhttp://www.bmva.org/bmvc/2004/papers/paper{\_}265.pdf},
year = {2004}
}
@article{Petitto1987,
abstract = {Two central assumptions of current models of language acquisition were addressed in this study: (1) knowledge of linguistic structure is "mapped onto" earlier forms of non-linguistic knowledge; and (2) acquiring a language involves a continuous learning sequence from early gestural communication to linguistic expression. The acquisition of the first and second person pronouns ME and YOU was investigated in a longitudinal study of two deaf children of deaf parents learning American Sign Language (ASL) as a first language. Personal pronouns in ASL are formed by pointing directly to the addressee (YOU) or self (I or ME), rather than by arbitrary symbols. Thus, personal pronouns in ASL resemble paralinguistic gestures that commonly accompany speech and are used prelinguistically by both hearing and deaf children beginning around 9 months. This provides a means for investigating the transition from prelinguistic gestural to linguistic expression when both gesture and language reside in the same modality. The results indicate that deaf children acquired knowledge of personal pronouns over a period of time, displaying errors similar to those of hearing children despite the transparency of the pointing gestures. The children initially (ages 10 and 12 months) pointed to persons, objects, and locations. Both children then exhibited a long avoidance period, during which one function of the pointing gesture (pointing to self and others) dropped out completely. During this period their language and cognitive development were otherwise entirely normal, and they continued to use other types of pointing (e.g., to objects). When pointing to self and others returned, it was marked with errors typical of hearing children; one child exhibited consistent pronoun reversal errors, thinking the YOU point referred to herself, while the other child exhibited reversal errors inconsistently. Evidence from experimental tasks conducted with the first child revealed that pronoun errors occurred in comprehension as well. Full control of the ME and YOU pronouns was not achieved until 25-27 months, around the same time when hearing children master these forms. Thus, the study provides evidence for a discontinuity in the child's transition from prelinguistic to linguistic communication. It is argued that aspects of linguistic structure and its acquisition appear to involve distinct, language-specific knowledge. ?? 1987.},
author = {Petitto, Laura A.},
doi = {10.1016/0010-0277(87)90034-5},
isbn = {0010-0277 (Print)$\backslash$n0010-0277 (Linking)},
issn = {00100277},
journal = {Cognition},
number = {1},
pages = {1--52},
pmid = {3691016},
title = {{On the autonomy of language and gesture: Evidence from the acquisition of personal pronouns in American sign language}},
volume = {27},
year = {1987}
}
@incollection{Padden2011,
abstract = {This chapter discusses a number of issues related to measuring poverty over time. It highlights some of the key normative decisions that have to be taken, in particular, the role of compensation over time (whether poverty spells can be compensated for by non-poverty spells); the issue of the discount rate (whether each spell should be given an equal weight); and the issue of the role of persistence (whether repeated spells should be given a higher weight). It offers a number of plausible poverty measures, each with different assumptions regarding these key issues, and shows how these insights can be used to construct a forward-looking measure of vulnerability. The chapter applies a number of these measures to data from rural Ethiopia, and shows that while correlations are high, there would still be considerable differences in ranking households by poverty according to different measures, especially those that have different views on the role of compensation},
archivePrefix = {arXiv},
arxivId = {arXiv:cond-mat/0402594v3},
author = {Padden, Carol A.},
booktitle = {Deaf around the World: The Impact of Language},
doi = {10.1093/acprof:oso/9780199732548.003.0001},
eprint = {0402594v3},
isbn = {9780199866359},
issn = {0873626X},
keywords = {Al-Sayyid Bedouin sign language,Middle east,Nicaraguan sign language,North America,Sign language distribution,Sign language geography,Sign language linguistics},
pmid = {16683413},
primaryClass = {arXiv:cond-mat},
title = {{Sign Language Geography}},
year = {2011}
}
@article{Shimada2001,
abstract = {This paper proposes a system for estimating arbitrary 3D human hand postures in real-time. It can accept not only pre-determined hand signs but also arbitrary postures and it works in a monocular camera environment. The estimation is based on a 2D image retrieval. More than 16,000 possible hand appearances are first generated from a given 3D shape model by rotating model joints and stored in an appearance database. Every appearance is tagged with its own joint angles which are used when the appearance was generated. By retrieving the appearance in the database well-matching to the input image contour, the joint angles of the input shape can be rapidly obtained. The search area is reduced by using an adjacency map in the database. To prevent tracking failures, a fixed number of the well-matching appearances are saved at every frame. After the multiple neighborhoods of the saved appearances are merged, the unified neighborhood is searched for the estimate efficiently by beam search. The posture estimates result from experimental examples are shown},
author = {Shimada, Nobutaka and Kimura, Kousuke and Shirai, Yoshiaki},
doi = {10.1109/RATFG.2001.938906},
isbn = {0769510744},
issn = {1530-1044},
journal = {Proceedings of IEEE ICCV Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems},
pages = {23--30},
title = {{Real-time 3D hand posture estimation based on 2D appearance retrieval using monocular camera}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=938906},
year = {2001}
}
@article{Fang2003,
abstract = {The major difficulty for large vocabulary sign language or gesture recognition lies in the huge search space due to a variety of recognized classes. How to reduce the recognition time without loss of accuracy is a challenge issue. In this paper, a hierarchical ...},
author = {Fang, Gaolin and Gao, Wen and Zhao, Debin},
file = {:Users/hadeelayoub/Desktop/LR Re-write/History/p125-fang.pdf:pdf},
journal = {ICMI '03: Proceedings of the 5th international conference on Multimodal interfaces},
keywords = {Gaussian mixture model,finite state machine,gesture recognition,hierarchical decision tree,sign language recognition},
title = {{Large vocabulary sign language recognition based on hierarchical decision trees}},
url = {http://portal.acm.org/citation.cfm?id=958432.958458},
year = {2003}
}
@article{Liddell1989a,
abstract = {Outlines phonological structure and processes of American Sign Language (ASL), covering: (1) sequential phenomena found in the production of individual signs; (2) the segmental phonetic transcription system; (3) paradigmatic and syntagmatic contrasts in ASL signs; (4) similarities between ASL and spoken language phonological processes; and (5) phonological effects on morphological processes. (20 references) (Author/CB)},
author = {Liddell, Scott K. and Johnson, Robert E.},
doi = {10.1353/sls.1989.0027},
issn = {0302-1475, 0302-1475},
journal = {Sign Language Studies},
pages = {195--278},
title = {{American Sign Language: The phonological base}},
volume = {64},
year = {1989}
}
@article{SidneyFels1993,
abstract = {To illustrate the potential of multilayer neural networks for adaptive interfaces, a VPL Data-Glove connected to a DECtalk speech synthesizer via five neural networks was used to implement a hand-gesture to speech system. Using minor variations of the standard backpropagation learning procedure, the complex mapping of hand movements to speech is learned using data obtained from a single ;speaker' in a simple training phase. With a 203 gesture-to-word vocabulary, the wrong word is produced less than 1{\%} of the time, and no word is produced about 5{\%} of the time. Adaptive control of the speaking rate and word stress is also available. The training times and final performance speed are improved by using small, separate networks for each naturally defined subtask. The system demonstrates that neural networks can be used to develop the complex mappings required in a high bandwidth interface that adapts to the individual user.},
author = {{Sidney Fels}, S. and Hinton, Geoffrey E.},
doi = {10.1109/72.182690},
file = {:Users/hadeelayoub/Desktop/LR Re-write/History/glove-talki.pdf:pdf},
isbn = {1045-9227 (Print)$\backslash$r1045-9227 (Linking)},
issn = {19410093},
journal = {IEEE Transactions on Neural Networks},
number = {1},
pages = {2--8},
pmid = {18267698},
title = {{Glove-Talk: A Neural Network Interface Between a Data-Glove and a Speech Synthesizer}},
volume = {4},
year = {1993}
}
@inproceedings{Appenrodt2010,
abstract = {In this paper we present our results of fingertip detection to realize an automatic gesture recognition system by using a multi stereo camera setup. The online framework detects automatically the hands and the face of the user based on depth and color information. To estimate the spatial position and the joints of fingers a 3D hand model was generated. We used the Iterative Closest Point (ICP) algorithm to calculate the distance error between the model and 3D input data. In addition a separation and evaluation of hand and fingertip movements was implemented. To solve the general problem of self-occlusion we developed a multi stereo camera system to increase the information density. The required calibration is presented by using ICP algorithm and Genetic Algorithm.},
author = {Appenrodt, J{\"{o}}rg and Handrich, Sebastian and Al-Hamadi, Ayoub and Michaelis, Bernd},
booktitle = {Proceedings of the 2010 International Conference of Soft Computing and Pattern Recognition, SoCPaR 2010},
doi = {10.1109/SOCPAR.2010.5685854},
isbn = {9781424478958},
keywords = {Computer vision {\&} image processing,Gesture recognition,Multi camera system,Pattern recognition,Stereo camera system},
pages = {35--40},
title = {{Multi stereo camera data fusion for fingertip detection in gesture recognition systems}},
year = {2010}
}
@article{Binh2006,
abstract = {Page 1. A NEW APPROACH DEDICATED TO HAND RECOGNITION Abstract The basic idea lies in the real-time generation of },
author = {Binh, N and Ejima, T},
journal = {{\ldots}},
title = {{A New Approach Dedicated to Hand Gesture Recognition}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4216392},
year = {2006}
}
@inproceedings{Yin2008,
abstract = {We address the feature selection problem for hidden Markov models (HMMs) in sequence classification. Temporal correlation in sequences often causes difficulty in applying feature selection techniques. Inspired by segmental k-means segmentation (SKS) [1], we propose Segmentally Boosted HMMs (SBHMMs), where the state-optimized features are constructed in a segmental and discriminative manner. The contributions are twofold. First, we introduce a novel feature selection algorithm, where the temporal dynamics are decoupled from the static learning procedure by assuming that the sequential data are piecewise independent and identically distributed. Second, we show that the SBHMM consistently improves traditional HMM recognition in various domains. The reduction of error compared to traditional HMMs ranges from 17{\%} to 70{\%} in American Sign Language recognition, human gait identification, lip reading, and speech recognition.},
author = {Yin, Pei and Essa, Irfan and Starrier, Thad and Rehg, James M.},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2008.4518031},
isbn = {1424414849},
issn = {15206149},
keywords = {Feature Extraction,Hidden Markov models,Pattern Recognition,Time-series},
pages = {2001--2004},
title = {{Discriminative feature selection for hidden Markov models using segmental boosting}},
year = {2008}
}
@article{Lillo-Martin1986,
abstract = {Null argument constructions in American Sign Lang (ASL) are investigated, {\&} it is concluded that there are two distinct types. Subjects {\&} Os can be phonologically unrealized in ASL, thus making it a Null Argument lang. However, it differs from the traditional Null Argument langs in that it has two types of null arguments. There are several V classes in ASL; of these, one class takes subject {\&} O agreement; another does not. This difference turns out to be crucial in the analysis of null arguments, {\&} indeed supports two of the analyses of null arguments given in the literature based on spoken langs. When a V is marked for agreement, null arguments are free, can act as null resumptive pronouns, {\&} thus are analyzed as null pronouns (pro), similar to the null pronouns found in Italian, Irish, etc. However, when there is no V agreement marking, null arguments cannot act as null resumptive pronouns. These null arguments can be analyzed as variables related to null topics, such as those found in Chinese. Thus, two kinds of null arguments, which have been theorized on the basis of spoken lang data, are found to coexist in ASL. 2 Figures, 43 References. AA},
author = {Lillo-Martin, Diane},
doi = {10.1007/BF00134469},
issn = {0167806X},
journal = {Natural Language and Linguistic Theory},
number = {4},
pages = {415--444},
title = {{Two kinds of null arguments in American Sign Language}},
volume = {4},
year = {1986}
}
@inproceedings{Utsumi1999,
abstract = {We propose a method of tracking 3D position, posture, and shapes of human hands from multiple-viewpoint images. Self-occlusion and hand-hand occlusion are serious problems in the vision-based hand tracking. Our system employs multiple-viewpoint and viewpoint selection mechanism to reduce these problems. Each hand position is tracked with a Kalman filler and the motion vectors are updated with image features in selected images that do not include hand-hand occlusion. 3D hand postures are estimated with a small number of reliable image features. These features are extracted based on distance transformation, and they are robust against changes in hand shape and self-occlusion. Finally, a {\&}ldquo;best view{\&}rdquo; image is selected for each hand for shape recognition. The shape recognition process is based on a Fourier descriptor. Our system can be used as a user interface device an a virtual environment, replacing glove-type devices and overcoming most of the disadvantages of contact-type devices},
author = {Utsumi, A and Ohya, Jun},
booktitle = {Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)},
doi = {10.1109/CVPR.1999.786980},
isbn = {0-7695-0149-4},
keywords = {Cameras,Humans,Image recognition,Kalman filters,Motion estimation,Robustness,Shape,Tracking,feature extraction,gesture recognition,hand tracking,hand-gesture tracking,image features,motion vectors,multiple cameras,multiple-viewpoint images,shape recognition,user interface,user interfaces},
pages = {473--478},
title = {{Multiple-hand-gesture tracking using multiple cameras}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=786980},
volume = {1},
year = {1999}
}
@article{Wang2009,
abstract = {Articulated hand-tracking systems have been widely used in virtual reality but are rarely deployed in consumer applications due to their price and complexity. In this paper, we propose an easy-to-use and inexpensive system that facilitates 3-D articulated user-input using the hands. Our approach uses a single camera to track a hand wear- ing an ordinary cloth glove that is imprinted with a custom pattern. The pattern is designed to simplify the pose estimation problem, allowing us to employ a nearest-neighbor approach to track hands at interactive rates. We describe several proof-of-concept applica- tions enabled by our system that we hope will provide a foundation for new interactions in modeling, animation control and augmented reality.},
author = {Wang, Robert Y. and Popovi{\'{c}}, Jovan},
doi = {10.1145/1531326.1531369},
isbn = {9781605587264},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {augmented reality,hand tracking,motion capture,user},
number = {3},
pages = {1},
title = {{Real-time hand-tracking with a color glove}},
volume = {28},
year = {2009}
}
@article{Freund1995,
abstract = {In the first part of the paper we consider the problem of dynamically$\backslash$n$\backslash$napportioning resources among a set of options in a worst-case on-line$\backslash$n$\backslash$nframework. The model we study can be interpreted as a broad, abstract$\backslash$n$\backslash$nextension of the well-studied on-line prediction model to a general$\backslash$n$\backslash$ndecision-theoretic setting. We show that the multiplicative weight-$\backslash$n$\backslash$nupdate LittlestoneWarmuth rule can be adapted to this model, yielding$\backslash$n$\backslash$nbounds that are slightly weaker in some cases, but applicable to a$\backslash$ncon-$\backslash$n$\backslash$nsiderably more general class of learning problems. We show how the$\backslash$n$\backslash$nresulting learning algorithm can be applied to a variety of problems,$\backslash$n$\backslash$nincluding gambling, multiple-outcome prediction, repeated games, and$\backslash$n$\backslash$nprediction of points in R{\^{}}{\{}n{\}}. In the second part of the paper we$\backslash$napply the$\backslash$n$\backslash$nmultiplicative weight-update technique to derive a new boosting algo-$\backslash$n$\backslash$nrithm. This boosting algorithm does not require any prior knowledge$\backslash$n$\backslash$nabout the performance of the weak learning algorithm. We also study$\backslash$n$\backslash$ngeneralizations of the new boosting algorithm to the problem of$\backslash$n$\backslash$nlearning functions whose range, rather than being binary, is an arbitrary$\backslash$n$\backslash$nfinite set or a bounded segment of the real line.},
author = {Freund, Y and Schapire, Re},
doi = {10.1006/jcss.1997.1504},
isbn = {3540591192},
issn = {00220000},
journal = {Computational learning theory},
keywords = {Adaboost,boosting,multi-class classification},
pages = {119--139},
pmid = {10394},
title = {{A desicion-theoretic generalization of on-line learning and an application to boosting}},
url = {http://link.springer.com/chapter/10.1007/3-540-59119-2{\_}166},
volume = {55},
year = {1995}
}
@article{Vogler1999,
abstract = {The major challenge that faces American Sign Language (ASL) recognition now is to develop methods that will scale well with increasing vocabulary size. Unlike in spoken languages, phonemes can occur simultaneously in ASL. The number of possible combinations of phonemes after enforcing linguistic constraints is approximately 5.5 × 10 8. Gesture recognition, which is less constrained than ASL recognition, suffers from the same problem. Thus, it is not feasible to train conventional hidden Markov models (HMMs) for large-scale ASL applications. Factorial HMMs and coupled HMMs are two extensions to HMMs that explicitly attempt to model several processes occuring in parallel. Unfortunately, they still require consideration of the combinations at training time. In this paper we present a novel approach to ASL recognition that aspires to being a solution to the scalability problems. It is based on parallel HMMs (PaHMMs), which model the parallel processes independently. Thus, they can also be trained independently, and do not require consideration of the different combinations at training time. We develop the recognition algorithm for PaHMMs and show that it runs in time polynomial in the number of states, and in time linear in the number of parallel processes. We run several experiments with a 22 sign vocabulary and demonstrate that PaHMMs can improve the robustness of HMM-based recognition even on a small scale. Thus, Pa-HMMs are a very promising general recognition scheme with applications in both gesture and ASL recognition. 1.},
author = {Vogler, Christian and Metaxas, Dimitris},
doi = {10.1109/ICCV.1999.791206},
isbn = {0-7695-0164-8},
journal = {Proceedings of the Seventh IEEE International Conference on Computer Vision},
pages = {116--122 vol.1},
title = {{Parallel hidden Markov models for American sign language recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=791206},
year = {1999}
}
@article{Oz2011,
abstract = {An American Sign Language (ASL) recognition system is being developed using artificial neural networks (ANNs) to translate ASL words into English. The system uses a sensory glove called the Cyberglove??? and a Flock of Birds?? 3-D motion tracker to extract the gesture features. The data regarding finger joint angles obtained from strain gauges in the sensory glove define the hand shape, while the data from the tracker describe the trajectory of hand movements. The data from these devices are processed by a velocity network with noise reduction and feature extraction and by a word recognition network. Some global and local features are extracted for each ASL word. A neural network is used as a classifier of this feature vector. Our goal is to continuously recognize ASL signs using these devices in real time. We trained and tested the ANN model for 50 ASL words with a different number of samples for every word. The test results show that our feature vector extraction method and neural networks can be used successfully for isolated word recognition. This system is flexible and open for future extension. ?? 2011 Elsevier Ltd. All rights reserved.},
author = {Oz, Cemil and Leu, Ming C.},
doi = {10.1016/j.engappai.2011.06.015},
isbn = {3-540-26208-3},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {ASL recognition,American Sign Language (ASL),Artificial Neural Networks (ANNs),Finger spelling recognition,Hand-shape recognition},
number = {7},
pages = {1204--1213},
title = {{American Sign Language word recognition with a sensory glove using artificial neural networks}},
volume = {24},
year = {2011}
}
@book{Johnston2007,
abstract = {This is first comprehensive introduction to the linguistics of Auslan, the sign language of Australia. Assuming no prior background in language study, it explores each key aspect of the structure of Auslan, providing an accessible overview of its grammar (how sentences are structured), phonology (the building blocks of signs), morphology (the structure of signs), lexicon (vocabulary), semantics (how meaning is created), and discourse (how Auslan is used in context). The authors also discuss a range of myths and misunderstandings about sign languages, provide an insight into the history and development of Auslan, and show how Auslan is related to other sign languages, such as those used in Britain, the USA and New Zealand. Complete with clear illustrations of the signs in use and useful further reading lists, this is an ideal resource for anyone interested in Auslan, as well as those seeking a clear, general introduction to sign language linguistics.},
author = {Johnston, Trevor and Schembri, Adam},
booktitle = {Reading},
doi = {10.1017/CBO9780511607479},
isbn = {9780521832977},
issn = {0167806X},
pages = {339},
title = {{Australian Sign Language (Auslan): An introduction to sign language linguistics}},
url = {http://discovery.ucl.ac.uk/10341/},
year = {2007}
}
@article{Freeman1995,
abstract = {One of the first works using edge oriantation histograms as features for image classification.},
author = {Freeman, Wt and Roth, Michal},
doi = {10.1.1.6.618},
journal = {International Workshop on Automatic Face and Gesture Recognition},
pages = {296--301},
title = {{Orientation histograms for hand gesture recognition}},
url = {http://aimm02.cse.ttu.edu.tw/class{\_}2009{\_}2/CV/OpenCV/References/Orientation histograms for hand gesture.pdf},
volume = {12},
year = {1995}
}
@article{Liang1998,
abstract = {In this paper, a large vocabulary sign language interpreter is presented with real-time continuous gesture recognition of sign language using a DataGloveTM. The most critical problem, end-point detection in a stream of gesture input is first solved and then statistical analysis is done according to 4 parameters in a gesture : posture, position, orientation, and motion. We have implemented a prototype system with a lexicon of 250 vocabularies in Taiwanese Sign Language (TWL). This system uses hidden Markov models (HMMs) for 51 fundamental postures, 6 orientations, and 8 motion primitives. In a signerdependent way, a sentence of gestures based on these vocabularies can be continuously recognized in real-time and the average recognition rate is 80.4{\%}.},
author = {Liang, Rung-huei and Ouhyoung, Ming},
doi = {10.1109/AFGR.1998.671007},
file = {:Users/hadeelayoub/Desktop/LR Re-write/History/5afbe2acc57530359c065227dbb634b2109b.pdf:pdf},
journal = {Third IEEE International Conference on Automatic Face and Gesture Recognition},
pages = {558--567},
title = {{A Real-time Continuous Gesture Recognition System for Sign Language}},
year = {1998}
}
@inproceedings{Vogler1997,
abstract = {We present an approach to continuous American sign language (ASL)$\backslash$nrecognition, which uses as input 3D data of arm motions. We use computer$\backslash$nvision methods for 3D object shape and motion parameter extraction and$\backslash$nan ascension technologies `Flock of Birds' interchangeably to obtain$\backslash$naccurate 3D movement parameters of ASL sentences, selected from a$\backslash$n53-sign vocabulary and a widely varied sentence structure. These$\backslash$nparameters are used as features for hidden Markov models (HMMs). To$\backslash$naddress coarticulation effects and improve our recognition results, we$\backslash$nexperimented with two different approaches. The first consists of$\backslash$ntraining context-dependent HMMs and is inspired by speech recognition$\backslash$nsystems. The second consists of modeling transient movements between$\backslash$nsigns and is inspired by the characteristics of ASL phonology. Our$\backslash$nexperiments verified that the second approach yields better recognition$\backslash$nresults},
author = {Vogler, Christian and Metaxas, Dimitris},
booktitle = {IEEE International Conference on Systems, Man and Cybernetics},
doi = {10.1109/ICSMC.1997.625741},
isbn = {0-7803-4053-1},
issn = {1062-922X},
pages = {156--161},
title = {{Adapting hidden Markov models for ASL recognition by using$\backslash$nthree-dimensional computer vision methods}},
volume = {1},
year = {1997}
}
@article{Starner1995a,
abstract = {Hidden Markov models (HMMs) have been used prominently and successfully in speech recognition and, more recently, in handwriting recognition. Consequently, they seem ideal for visual recognition of complex, structured hand gestures such as are found in sign language. We describe a real-time HMM-based system for recognizing sentence level American Sign Language (ASL) which attains a word accuracy of 99.2{\%} without explicitly modeling the fingers},
author = {Starner, T and Pentland, A},
doi = {10.1109/ISCV.1995.477012},
isbn = {0818671904},
journal = {Proceedings of International Symposium on Computer Vision ISCV},
pages = {265--270},
title = {{Real-time American Sign Language recognition from video using hidden Markov models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=477012},
year = {1995}
}
@book{Lucas2001,
abstract = {An introduction to research methods {\&} findings in sociolinguistic variation distinguishes between internal (linguistic) {\&} external (demographic) constraints on variable linguistic units {\&} between synchronic {\&} diachronic variation. Investigations of variation in sign languages are surveyed. A review of research in American Sign Language (ASL) sketches lexical, phonological, morphological, syntactic, {\&} diachronic processes; studies of other sign languages are noted. Variation in sign vs spoken languages is compared with respect to variable units {\&} processes {\&} to internal {\&} social constraints. Three recent studies of variation in ASL are abstracted: R. Hoppes's (1998) study of pinky extension, S. Collins {\&} K. Petronio's (1998) study of tactile (Deaf-Blind) ASL, {\&} C. R. Lucas, et al's (2001) study of variation in the sign for "deaf." Methodological considerations for investigating sign language variation are explained. Research in sign language variation should be able to account for the differences of the environments {\&} structures from those of spoken languages. 7 Tables, 4 Figures. E. Taylor},
author = {Lucas, Ceil and Bayley, Robert and Valli, Clayton and Rose, Mary},
booktitle = {The Sociolinguistics of Sign Languages},
doi = {http://dx.doi.org/10.1017/CBO9780511612824},
isbn = {9780511612824},
pages = {61--111},
pmid = {13264146},
title = {{Sociolinguistic variation}},
url = {http://ebooks.cambridge.org/ebook.jsf?bid=CBO9780511612824},
year = {2001}
}
@article{Neidle1998,
abstract = {ASL syntax makes essential use of specific non-manual expressions of syntactic features (e.g., +neg, +wh) that co-occur with manual signs. These markings occur obligatorily with manual material contained in the node of origin and optionally extend over the c-command domain of that node, thus providing important evidence for hierarchical structure. In this article, we show that agreement features, both within the clause and the noun phrase, also have non-manual correlates that exhibit the predicted distribution. Interestingly, transitive IPs and possessive DPs pattern together in their manifestation of agreement marking, while intransitive IPs pattern with non-possessive DPs.},
author = {Neidle, Carol and Bahan, Benjamin and MacLaughlin, Dawn and Lee, Robert G and Kegl, Judy},
doi = {10.1111/1467-9582.00034},
issn = {1467-9582},
journal = {Studia Linguistica},
number = {3},
pages = {191--226},
title = {{Realizations of syntactic agreement in American sign language: Similarities between the clause and the noun phrase}},
url = {http://dx.doi.org/10.1111/1467-9582.00034},
volume = {52},
year = {1998}
}
@article{Cooper2012,
abstract = {This paper discusses sign language recognition using linguistic sub-units. It presents three types of sub-units for consideration; those learnt from appearance data as well as those inferred from both 2D or 3D tracking data. These sub-units are then combined using a sign level classifier; here, two options are presented. The first uses Markov Models to encode the temporal changes between sub-units. The second makes use of Sequential Pattern Boosting to apply discriminative feature selection at the same time as encoding temporal information. This approach is more robust to noise and performs well in signer independent tests, improving results from the 54{\{}{\%}{\}} achieved by the Markov Chains to 76{\{}{\%}{\}}.},
author = {Cooper, H and Ong, E J and Pugeault, N and Bowden, R},
doi = {10.1007/978-3-540-75773-310; Cooper, H., Bowden, R., Sign language recognition using linguistically derived sub-units (2010) Proceedings of the Language Resources and Evaluation ConferenceWorkshop on the Representation and Processing of Sign Languages : Corpora and Sign Languages Technologies, Valetta, Malta, May17-23; Elliott, R., Glauert, J., Kennaway, J., Parsons, K., (2001) D5-2: SiGML Definition, , ViSiCAST Project working document; Ershaed, H., Al-Alali, I., Khasawneh, N., Fraiwan, M., An},
file = {:Users/hadeelayoub/Desktop/LR Re-write/History/cooper12a.pdf:pdf},
isbn = {15324435 (ISSN)},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {ARTIFICIAL INTELLIGENCE,AUTOMATION {\{}{\&}{\}} CONTROL SYSTEMS,COMPUTER SCIENCE,HIDDEN MARKOV-MODELS,data set,depth cameras,sequential pattern boosting,sign language recognition,signer independence,sub-units},
pages = {2205--2231},
title = {{Sign Language Recognition using Sub-Units}},
url = {http://ncsu.summon.serialssolutions.com/2.0.0/link/0/eLvHCXMwY2BQMDZKNktLTkuzTE4zNgGmCVCVY2KYamaUZJJkYmGCOpCDVJq7CTEwpeaJMmi7uYY4e-gi1wPxBZCzF-LBbXhzE2C7G9y6NRRj4E0ELQXPKwFvGUsBAFuLIDo},
volume = {13},
year = {2012}
}
@article{Kawai1985,
abstract = {We have developed a system which can recognize speech and generate the corresponding animation-like sign language sequence in real-time. The system is implemented in a popular personal computer. This has three video RAM (VRAM) and a voice recognition board which can recognize only the registered voice of a specific speaker. At present, forty sign language patterns and fifty finger spellings are stored on two floppy disks. Each sign pattern is composed of one to four sub-patterns. That is, if the pattern is composed of one subpattern, it is displayed as a still pattern. If not, it is displayed as a motion pattern. Japanese has fifty Kana characters. Each character has a corresponding finger spelling. Some of them are also displayed as a motion pattern composed of three sub-patterns. When a text is input from a keyboard or voice recognition circuit board (unit), a corresponding pattern sequence composed of a mixture of sign language patterns and finger spelling patterns is generated and displayed on a CRT. The data of each subpattern are read from a floppy disk and transferred to one of the VRAM (640 × 200 × 1 bits). Since the computer has three VRAM, it is possible to read and transfer the data even when a preceding pattern is still being displayed on the CRT. Thus, spoken natural language sentences can be displayed in animation form by switching the VRAM. This system will help communication between deaf-mute and healthy persons. In order to display in high speed, most programs are written in a machine language. {\textcopyright} 1985.},
author = {Kawai, H and Tamura, S},
isbn = {00313203 (ISSN)},
journal = {Pattern Recognition},
keywords = {ANIMATION-LIKE SIGN LANGUAGE SEQUENCE,Animation,DEAF-AND-MUTE SIGN LANGUAGE,FINGER SPELLINGS,INFORMATION SCIENCE,Personal computer,SIGN LANGUAGE PATTERNS,SPEECH - Recognition,Sign language,Welfare apparatus},
number = {3-4},
pages = {199--205},
title = {{Deaf-and-mute sign language generation system}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-0021976880{\&}partnerID=40{\&}md5=9fe8450d3c50307e5ed88e01e1c1fcae},
volume = {18},
year = {1985}
}
@book{Neidle2000,
abstract = {Recent research on the syntax of signed languages has revealed that, apart from some modality-specific differences, signed languages are organized according to the same underlying principles as spoken languages. This volume in the Language, Speech, and Communication series addresses the organization {\&} distribution of functional categories in American Sign Language (ASL), focusing on tense, agreement, {\&} wh-constructions. Signed languages provide illuminating evidence about functional projections of a kind unavailable in the study of spoken languages. Along with manual signing, crucial information is expressed by specific movements of the face {\&} upper body. It is argued that such nonmanual markings are often direct expressions of abstract syntactic features. This distribution {\&} intensity of these markings provide information about the location of functional heads {\&} the boundaries of functional projections. It is shown how evidence from ASL is useful for evaluating a number of recent theoretical proposals on, among other things, the status of syntactic agreement projections {\&} constraint on phrase structure {\&} the directionality of movement. 1 Table, 7 Figures, Bibliog. Adapted from the source document},
author = {Neidle, Carol and Kegl, Judy and MacLaughlin, Dawn and Bahan, Benjamin and Lee, Robert G},
booktitle = {x+229pp, Cambridge: Massachusetts Instit Technology Press, 2000},
isbn = {0262140675},
keywords = {*Agreement (01230),*American Sign Language (02350),*Functional Heads (26443),*Grammatical Categories (28750),*Nonverbal Communication (58500),*Tense (88500),*Wh Phrases (96650),4310: syntax; syntax,5810: nonverbal communication; human nonverbal lan,book},
title = {{THE SYNTAX OF AMERICAN SIGN LANGUAGE: FUNCTIONAL CATEGORIES AND HIERARCHICAL STRUCTURE}},
url = {http://search.proquest.com/docview/85536737?accountid=14630{\%}5Cnhttp://139.165.41.136:3210/sfxulg??url{\_}ver=Z39.88-2004{\&}rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:book{\&}genre=book{\&}sid=ProQ:Linguistics+and+Language+Behavior+Abstracts+(LLBA){\&}atitle={\&}title=THE+SYNTAX+OF+},
year = {2000}
}
@article{Isaacs2004,
abstract = { In the foreseeable future, gestured inputs will be widely used in human-computer interfaces. This paper describes our initial attempt at recognizing 2D hand poses for application in video-based human-computer interfaces. Specifically, this research focuses on 2-D image recognition utilizing an evolved wavelet-based feature vector. We have developed a two layer feed-forward neural network that recognizes the 24 static letters in the American sign language (ASL) alphabet using still input images. Thus far, two wavelet-based decomposition methods have been used. The first produces an 8-element real-valued feature vector and the second a 18-element feature vector. Each set of feature vectors is used to train a feed-forward neural network using Levenberg-Marquardt training. The system is capable of recognizing instances of static ASL fingerspelling with 99.9{\%} accuracy with an SNR as low as 2. We conclude by describing issues to be resolved before expanding the corpus of ASL signs to be recognized.},
author = {Isaacs, J. and Foo, S.},
doi = {10.1109/SSST.2004.1295634},
isbn = {0-7803-8281-1},
issn = {0094-2898},
journal = {Thirty-Sixth Southeastern Symposium on System Theory, 2004. Proceedings of the},
pages = {132--136},
title = {{Hand pose estimation for American sign language recognition}},
year = {2004}
}
@article{Tamura1988,
abstract = {This paper describes a method of classifying single view deaf-and-mute sign language motion images. We suppose the sign language word is composed of a time sequence of units called cheremes. The chereme is described by handshape, movement, and location of the hand, which can be said to express the 3-D features of the sign language. First, a dictionary for recognizing the sign language is made based on the cheremes. Then, the macro 2-D features of the location of a hand and its movement are extracted from the red component of the input color image sequence. Further, the micro 2-D features of the shape of the hand are also extracted if necessary. The 3-D feature descriptions of the dictionary are converted into 2-D image features, and the input sign language image is classified according to the extracted features of the 2-D image. ?? 1988.},
author = {Tamura, Shinichi and Kawasaki, Shingo},
doi = {10.1016/0031-3203(88)90048-9},
isbn = {00313203 (ISSN)},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Human motion,Image processing,Image sequence,Motion image,Pattern classification,Sign language},
number = {4},
pages = {343--353},
title = {{Recognition of sign language motion images}},
volume = {21},
year = {1988}
}
@article{Sturman1994,
abstract = {Clumsy intermediary devices constrain our interaction with computers and their applications. Glove-based input devices let us apply our manual dexterity to the task. We provide a basis for understanding the field by describing key hand-tracking technologies and applications using glove-based input. The bulk of development in glove-based input has taken place very recently, and not all of it is easily accessible in the literature. We present a cross-section of the field to date. Hand-tracking devices may use the following technologies: position tracking, optical tracking, marker systems, silhouette analysis, magnetic tracking or acoustic tracking. Actual glove technologies on the market include: Sayre glove, MIT LED glove, Digital Data Entry Glove, DataGlove, Dexterous HandMaster, Power Glove, CyberGlove and Space Glove. Various applications of glove technologies include projects into the pursuit of natural interfaces, systems for understanding signed languages, teleoperation and robotic control, computer-based puppetry, and musical performance.},
author = {Sturman, David J. and Zeltzer, David},
doi = {10.1109/38.250916},
isbn = {0272-1716},
issn = {02721716},
journal = {IEEE Computer Graphics and Applications},
number = {1},
pages = {30--39},
title = {{A Survey of Glove-based Input}},
volume = {14},
year = {1994}
}
@article{Premaratne2013a,
abstract = {Dynamic hand gesture tracking and recognition system can simplify the way humans interact with computers and many other non-critical consumer electronic equipments. This system is based on the well-known "Wave Controller" technology developed at the University of Wollongong [1-3] and certainly a step forward in video gaming and consumer electronics control interfaces. Currently, computer interfacing mainly involves keyboard, mouse, joystick or gaming wheels and occasionally voice recognition for user input. These modes of interaction have constrained the artistic ability of many users, as they are required to respond to the computer through pressing buttons or moving other apparatus. Voice recognition is seen as unreliable and impractical in areas where more than one user is present. All these drawbacks can be tackled by using a reliable hand gesture tracking and recognition system based on both Lucas-Kanade and Moment Invariants approaches. This will facilitate interaction between users and computers and other consumer electronic equipments in real time. This will further enhance the user experience as users are no longer have any physical connection to the equipment being controlled. In this research, we have compared our proposed moment invariant based algorithm with template based and Fourier descriptor based methods to highlight the advantages and limitations of the proposed system. ?? 2012 Elsevier B.V.},
author = {Premaratne, Prashan and Ajaz, Sabooh and Premaratne, Malin},
doi = {10.1016/j.neucom.2011.11.039},
isbn = {09252312 (ISSN)},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Computer human interaction (HCI),Dynamic gesture recognition,Lucas-Kanade algorithm,Moment invariants,Support vector machines,Tracking},
pages = {242--249},
title = {{Hand gesture tracking and recognition system using Lucas-Kanade algorithms for control of consumer electronics}},
volume = {116},
year = {2013}
}
@phdthesis{Collins2004,
abstract = {This PDE discusses an aspect of linguistic use of adverbial morphemes as applied to a single case study of Tactile American Sign Language (TASL) as used by some American Deaf-Blind signers. TASL, a variation of the visual language recognized as American Sign Language (ASL), is not visually based. Significantly, the term TASL, while descriptive of the language used by deaf-blind persons, is not officially recognized among the members of this community. For the purposes of this study, the use of the term TASL describes not only an approach towards building a bridge of understanding of the cultural norms and language habits of deaf-blind persons, but as well, this study affords a locus for the improvement of the interpreting process for the American deaf-blind community.                 In ASL adverbial morphemes occur on the face and are non-manual signals that the Deaf-Blind signer does not see. This requires the ASL signer to make a slight modification, from these "invisible" non-manual morphemes to a tactile morpheme.                 The researcher presents a structural analysis of a conversation between two Deaf-Blind subjects with Usher's Syndrome Type I who have used TASL for at least ten years. The study is based on a 50-minute videotaped conversation utilizing multiple video views of key angles for a detailed analysis of tactile components, non-manual signals and signing space.                 Accrued data concentrates on six fundamental features of adverbial morphemes intrinsic to TASL: manner/degree, time, duration, purpose, frequency, and place/position/direction. A total of 284 sign sequences were observed and analyzed comparing the tactile signing with the same message signed in visual American Sign Language.},
author = {Collins, Steven Douglas},
booktitle = {ProQuest Dissertations and Theses},
isbn = {9780496028955; 0496028952},
keywords = {0290:Linguistics,Adverbal morphemes,American Sign Language,Language, literature and linguistics,Linguistics,Tactile American Sign Language},
pages = {125--125 p.},
title = {{Adverbal morphemes in Tactile American Sign Language}},
url = {http://proxyga.wrlc.org/login?url=http://search.proquest.com/docview/305050140?accountid=27346{\%}5Cnhttp://fw9ek8vp6c.search.serialssolutions.com/?ctx{\_}ver=Z39.88-2004{\&}ctx{\_}enc=info:ofi/enc:UTF-8{\&}rfr{\_}id=info:sid/ProQuest+Dissertations+{\%}26+Theses+Global{\&}rft{\_}va},
year = {2004}
}
@article{LaViola1999,
abstract = {This paper surveys the use of hand postures and gestures as a mechanism for inter- action with computers, describing both the various techniques for performing accurate recognition and the technological aspects inherent to posture- and gesture-based inter- action. First, the technological requirements and limitations for using hand postures and gestures are described by discussing both glove-based and vision-based recogni- tion systems along with advantages and disadvantages of each. Second, the various types of techniques used in recognizing hand postures and gestures are compared and contrasted. Third, the applications that have used hand posture and gesture interfaces are examined. The survey concludes with a summary and a discussion of future re- search directions.},
author = {LaViola, J},
isbn = {CS-99-11},
journal = {Brown University, Providence, RI},
title = {{A survey of hand posture and gesture recognition techniques and technology}},
url = {http://www.pervasive.jku.at/Teaching/{\_}2012SS/SeminarausPervasiveComputing/Begleitmaterial/Related Work/1999{\_}A Survey of Hand Posture and Gesture Recognition Techniques and Technology{\_}LaViola.pdf},
year = {1999}
}
@article{Vogler2004,
abstract = {In this paper we present a framework for recognizing American Sign Language (ASL). The main challenges in developing scalable recognition systems are to de- vise the basic building blocks from which to build up the signs, and to handle simultaneous events, such as signs where both the hand moves and the handshape changes. The latter challenge is particularly thorny, because a naive approach to handling them can quickly result in a combinatorial explosion. We loosely follow the Movement-Hold model to devise a breakdown of the signs into their constituent phonemes, which provide the fundamental building blocks. We also show how to integrate the handshape into this breakdown, and discuss what handshape representation works best. To handle simultaneous events, we split up the signs into a number of channels that are independent from one another. We validate our framework in experiments with a 22-sign vocabulary and up to three channels.},
author = {Vogler, Christian and Metaxas, Dimitris},
doi = {10.1007/978-3-540-24598-8_23},
isbn = {3-540-21072-5},
journal = {Artificial Intelligence},
pages = {1--13},
title = {{Handshapes and movements : Multiple-channel ASL recognition}},
year = {2004}
}
@article{BishopMichele;Hicks2005,
abstract = {Different aspects of bilingualism have been studied all over the world, and the studies have looked at a wide range of topics in spoken- language bilinguals such as patterns of code switching, the role of code switching in community life, the success or failure of bilingual education, second-language learning and gender, as well as many other issues focusing on single-modality bilinguals. These studies are often not applicable to studies of bimodal bilingualism, in which the subjects know a sign language from birth and the spoken language of the larger, hearing society. The study of bilingualism in hearing people from deaf families offers an opportunity to analyze the way that native users of both a signed and a spoken language combine aspects of both languages simultaneously (code blending). The lower status of American Sign Language (ASL) in relation to English may also contribute to how bimodal bilinguals view and use their languages. Unlike spoken-language bilinguals, who must stop one language before beginning another, a bimodal bilingual is able to speak and sign at the same time. This linguistic capability informs and expands the field of bilingualism as well as areas such as discourse analysis and the role of code blending as a cultural identifier. This preliminary research focuses on emails taken from a forum on the Internet for hearing people with deaf parents. Two hundred and seventy five lines from one hundred emails were collected and analyzed. The study shows evidence of strong grammatical influence from ASL in these emails (an absence of overt subjects, overt objects, determiners, copulas, and prepositions) as well as unique structures (nonstandard verb inflections, overgeneralization of the letter s, and syntactic calquing). There is also a strong tendency to use English to “describe” an ASL sign (i.e., “My father fork-in-throat”). The meaning of the sign FORK-IN-THROAT is “stuck,” but the bilingual chooses to use the visual description of the sign instead of the lexical equivalent in English (note the absence of the copula). The overall results of this analysis are compared to Internet Relay Chat and TDD writings.},
author = {{Bishop, Michele; Hicks}, Sherry},
doi = {10.1353/sls.2005.0001},
issn = {1533-6263},
journal = {Sign Language Studies},
number = {2},
pages = {188--230},
title = {{Bimodal Bilingualism in Hearing Adults from Deaf Families}},
url = {http://mwbdvjh.muse.jhu.edu/journals/sign{\_}language{\_}studies/v005/5.2bishop.pdf},
volume = {5},
year = {2005}
}
@article{Zimmerman1982,
abstract = {An optical flex sensor is provided and consists of a fexible tube having two ends, a reflective interior wall within the fexible tube and a light source placed within one end of the fexible tube and a photosensitive detec tor placed within the other end of the fexible tube to detect a combination of direct light rays and reflected rays when the fexible tube is bent.},
author = {Zimmerman, TG},
journal = {US Patent 4,542,291},
pages = {1--3},
title = {{Optical flex sensor}},
url = {http://www.google.com/patents?hl=en{\&}lr={\&}vid=USPAT4542291{\&}id=6No6AAAAEBAJ{\&}oi=fnd{\&}dq=Optical+Flex+Sensor{\&}printsec=abstract},
year = {1982}
}
@inproceedings{Utsumi1996,
abstract = {We describe a method to detect hand position, posture and finger bendings using multiple camera images. Stable detection can be achieved by using skeleton images, and this is confirmed through experiments. This system can be used as a user interface device in a virtual environment, replacing glove-type devices and overcoming most of the disadvantages of contact-type devices. Future work includes verification of the method's availability as a gesture-based man-machine interface system. {\textcopyright} 1996 IEEE.},
author = {Utsumi, A and Miyasato, T and Kishino, F and Nakatsu, R},
booktitle = {13th International Conference on Pattern Recognition, ICPR 1996},
doi = {10.1109/ICPR.1996.546108},
isbn = {10514651 (ISSN); 081867282X (ISBN); 9780818672828 (ISBN)},
keywords = {Contact type,Gesture recognition,Hand positions,Hand-gesture recognition,Man-machine interface,Multiple camera images,Multiple cameras,Pattern recognition,User interface devices,User interfaces,Virtual reality},
pages = {667--671},
title = {{Hand gesture recognition system using multiple cameras}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-58149110477{\&}partnerID=40{\&}md5=0572d732a29630baba3da19fb97db039},
volume = {1},
year = {1996}
}
@inproceedings{Waldron1994,
abstract = {In this paper we present the results of relabelling a self organizing map (SOM) to increase the dynamic manual signs it can recognize. Relabelling exploits the global ordering of self organizing map and abrogates the need for retraining, thereby reducing the computational costs and increasing the recognition ability of the network. This relabelling technique was applied to a dynamic sign recognition system to increase the recognition vocabulary from 10 to 14 signs. The data was collected from a person wearing a DataGlove with a Polhemus sensor and signing the 14 signs. The sampled hand data over the duration of sign was fed to phonemic recognition modules and the collective outputs of these modules were fed to the sign recognition module consisting of a relabelled self organizing network. The results showed that the overall recognition rate of the relabelled network was 84{\%} as compared to 86{\%} for the retrained network. Further, it was found that the dynamic sampling of the signs made the movement phoneme module unnecessary.},
author = {Waldron, Manjula B and Kim, Soowon},
booktitle = {Proc. ICNN'94, International Conference on Neural Networks},
pages = {2885--2889},
title = {{Increasing Manual Sign Recognition Vocabulary through Relabelling}},
year = {1994}
}
@article{Cootes1992,
abstract = {We describe 'Active Shape Models' which iteratively adapt to refine estimates of the pose, scale and shape of models of image objects. The method uses flexible models derived from sets of training examples. These models, known as Point Distribution Models, represent objects as sets of labelled points. An initial estimate of the location of the model points in an image is improved by attempting to move each point to a better position nearby. Adjustments to the pose variables and shape parameters are calculated. Limits are placed on the shape parameters ensuring that the example can only deform into shapes conforming to global constraints imposed by the training set. An iterative procedure deforms the model example to find the best fit to the image object. Results of applying the method are described. The technique is shown to be a powerful method for refining estimates of object shape and location},
author = {Cootes, T. F. and Taylor, C. J.},
doi = {10.5244/C.6.28},
isbn = {3-540-19777-X},
journal = {Procedings of the British Machine Vision Conference 1992},
pages = {28.1--28.10},
title = {{Active Shape Models - 'Smart Snakes'}},
url = {http://www.bmva.org/bmvc/1992/bmvc-92-028.html},
year = {1992}
}
@article{Kang1993,
abstract = {Many of the tasks that are potential candidates for automation involve grasping. The authors are interested in the programming of robots to perform grasping tasks. To do this, the assembly plan from observation (APO) paradigm is adopted, where the key idea is to enable a system to observe a human performing a grasping task, understand it, and perform the task with minimal human intervention. A grasping task is composed of three phases: pregrasp phase, static grasp phase, and manipulation phase. The first step in recognizing a grasping task is identifying the grasp itself (within the static grasp phase). The proposed strategy of identifying the grasp is to map the low-level hand configuration to increasingly more abstract grasp descriptions. The abstract grasp descriptions are useful because they are manipulator-independent. To achieve the mapping, a grasp representation is introduced that is called the contact web, which is composed of a pattern of effective contact points between the hand and the object. A grasp taxonomy based on the contact web is also proposed as a tool to systematically identify a grasp. The grasp can be described at higher conceptual levels using a certain mapping function that results in an index called the grasp cohesive index. This index can be used to identify the grasp. Results from grasping experiments show that it is possible to distinguish between various types of grasps using the proposed contact web, grasp taxonomy and grasp cohesive index.},
author = {Kang, Sing Bing and Ikeuchi, Katsushi},
doi = {10.1109/70.246054},
isbn = {1042-296X},
issn = {1042296X},
journal = {IEEE Transactions on Robotics and Automation},
number = {4},
pages = {432--443},
title = {{Toward Automatic Robot Instruction from Perception—Recognizing a Grasp from Observation}},
volume = {9},
year = {1993}
}
@book{Costello2008,
abstract = {The Random House Webster's Unabridged American Sign Language Dictionary is a treasury of over 5,600 signs for the novice and experienced user alike. It includes complete descriptions of each sign, plus full-torso illustrations. There is also a subject index for easy reference as well as alternate signs for the same meaning.},
author = {Costello, Elaine},
booktitle = {Random House Reference},
isbn = {9780553584745},
pages = {1200},
title = {{Random House Webster's American Sign Language Dictionary}},
url = {http://books.google.com.mx/books?id=57Zrj4ELEsQC},
year = {2008}
}
@article{Armstrong2009,
abstract = {Presents a commemoration of William C. Stokoe, one of the most influential language scholars of the twentieth century. He is often described as having "discovered" American Sign Language (ASL) or as having "proved" that ASL is a language. His achievements with respect to the value of signed languages were essentially fourfold. Stokoe's first achievement was to realize that the signed language his students used among themselves had all the important characteristics common to spoken languages and that it had the same potential for human communication. His second achievement was to devise a descriptive system that would convince language scholars of these facts. This was what gave him the legitimacy to pursue his third achievement—convincing much of the general public and the educational establishment of the human and educational value of allowing deaf children to communicate in natural signed languages. His fourth grand achievement was then to apply what he had learned from the study of signed languages to the larger problems of the nature and evolution of the human capacity for language. Stokoe owned, edited, and published this journal himself for more than twenty years, and the journal chronicles the growth and maturation of the fields of signed language research and deaf studies. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
author = {Armstrong, David F and Karchmer, Michael A},
doi = {10.1353/sls.0.0027},
isbn = {3180547464},
issn = {0302-1475},
journal = {Sign Language Studies},
keywords = {American Sign Language,College Teachers,Deaf,Linguistics,Scientists,Sign Language,William C. Stokoe,communication,deaf people,signed languages},
number = {4},
pages = {389--397},
title = {{William C. Stokoe and the study of signed languages: Commemoration}},
url = {http://search.ebscohost.com.proxy-ub.rug.nl/login.aspx?direct=true{\&}db=psyh{\&}AN=2009-11189-002{\&}site=ehost-live{\&}scope=site},
volume = {9},
year = {2009}
}
@article{Tamura1988a,
abstract = {This paper describes a method of classifying single view deaf-and-mute sign language motion images. We suppose the sign language word is composed of a time sequence of units called cheremes. The chereme is described by handshape, movement, and location of the hand, which can be said to express the 3-D features of the sign language. First, a dictionary for recognizing the sign language is made based on the cheremes. Then, the macro 2-D features of the location of a hand and its movement are extracted from the red component of the input color image sequence. Further, the micro 2-D features of the shape of the hand are also extracted if necessary. The 3-D feature descriptions of the dictionary are converted into 2-D image features, and the input sign language image is classified according to the extracted features of the 2-D image. ?? 1988.},
author = {Tamura, Shinichi and Kawasaki, Shingo},
doi = {10.1016/0031-3203(88)90048-9},
isbn = {00313203 (ISSN)},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Human motion,Image processing,Image sequence,Motion image,Pattern classification,Sign language},
number = {4},
pages = {343--353},
title = {{Recognition of sign language motion images}},
volume = {21},
year = {1988}
}
@article{James1994,
abstract = {. This paper presents a method for recognizing human-hand gestures using a model-based approach. A Finite State Machine is used to model four qualitatively distinct phases of a generic gesture. Fingertips are tracked in multiple frames to compute motion trajectories, which are then used for finding the start and stop position of the gesture. Gestures are represented as a list of vectors and are then matched to stored gesture vector models using table lookup based on vector displacements. Results are presented showing recognition of seven gestures using images sampled at 4 Hz on a SPARC-1 without any special hardware. The seven gestures are representatives for actions of Left, Right, Up, Down, Grab, Rotate, and Stop. 1 Introduction It is essential for computer systems to possess the ability to recognize meaningful gestures if they are to interact naturally with people. Humans use gestures in daily life as a means of communication, e.g., pointing to an object to bring someone{\&}apos;s attentio...},
author = {James, Davis and Mubarak, Shah},
doi = {10.1007/3-540-57956-7_37},
isbn = {978-3-540-48398-4},
journal = {European Conference on Computer Vision , Stockholm, Sweden},
keywords = {Left,Right,Up,Down,Grab,Rotate,and Stop,are representatives for actions of},
title = {{Recognizing Hand Gestures}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=?doi=10.1.1.43.9743},
year = {1994}
}
@article{Imagawa2000,
abstract = {A sign language recognition system is required to use information from both global features, such as hand movement and location, and local features, such as hand shape and orientation. In this paper, we present an adequate local feature recognizer for a sign language recognition system. Our basic approach is to represent the hand images extracted from sign-language images as symbols which correspond to clusters by a clustering technique. The clusters are created from a training set of extracted hand images so that a similar appearance can be classified into the same cluster on an eigenspace. The experimental results indicate that our system can recognize a sign language word even in two-handed and hand-to-hand contact cases. {\textcopyright} 2000 IEEE.},
author = {Imagawa, K and Matsuo, H and Taniguchi, R I and Arita, D and Lu, S and Igi, S},
doi = {10.1109/ICPR.2000.903050},
isbn = {10514651 (ISSN)},
issn = {1051-4651},
journal = {Proceedings - International Conference on Pattern Recognition},
number = {4},
pages = {849--853},
title = {{Recognition of local features for camera-based sign language recognition system}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-33750921419{\&}partnerID=40{\&}md5=d16917cbb9a9bdef7d9eaef77df218ee},
volume = {15},
year = {2000}
}
@article{Premaratne2013b,
abstract = {Dynamic hand gesture tracking and recognition system can simplify the way humans interact with computers and many other non-critical consumer electronic equipments. This system is based on the well-known "Wave Controller" technology developed at the University of Wollongong [1-3] and certainly a step forward in video gaming and consumer electronics control interfaces. Currently, computer interfacing mainly involves keyboard, mouse, joystick or gaming wheels and occasionally voice recognition for user input. These modes of interaction have constrained the artistic ability of many users, as they are required to respond to the computer through pressing buttons or moving other apparatus. Voice recognition is seen as unreliable and impractical in areas where more than one user is present. All these drawbacks can be tackled by using a reliable hand gesture tracking and recognition system based on both Lucas-Kanade and Moment Invariants approaches. This will facilitate interaction between users and computers and other consumer electronic equipments in real time. This will further enhance the user experience as users are no longer have any physical connection to the equipment being controlled. In this research, we have compared our proposed moment invariant based algorithm with template based and Fourier descriptor based methods to highlight the advantages and limitations of the proposed system. ?? 2012 Elsevier B.V.},
author = {Premaratne, Prashan and Ajaz, Sabooh and Premaratne, Malin},
doi = {10.1016/j.neucom.2011.11.039},
isbn = {09252312 (ISSN)},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Computer human interaction (HCI),Dynamic gesture recognition,Lucas-Kanade algorithm,Moment invariants,Support vector machines,Tracking},
pages = {242--249},
title = {{Hand gesture tracking and recognition system using Lucas-Kanade algorithms for control of consumer electronics}},
volume = {116},
year = {2013}
}
@article{Dipietro2008,
abstract = {Hand movement data acquisition is used in many engineering applications ranging from the analysis of gestures to the biomedical sciences. Glove-based systems represent one of the most important efforts aimed at acquiring hand movement data. While they have been around for over three decades, they keep attracting the interest of researchers from increasingly diverse fields. This paper surveys such glove systems and their applications. It also analyzes the characteristics of the devices, provides a road map of the evolution of the technology, and discusses limitations of current technology and trends at the frontiers of research. A foremost goal of this paper is to provide readers who are new to the area with a basis for understanding glove systems technology and how it can be applied, while offering specialists an updated picture of the breadth of applications in several engineering and biomedical sciences areas.},
author = {Dipietro, Laura and Sabatini, Angeloi M. and Dario, Paolo},
doi = {10.1109/TSMCC.2008.923862},
isbn = {1931971471},
issn = {10946977},
journal = {IEEE Transactions on Systems, Man and Cybernetics Part C: Applications and Reviews},
keywords = {Gestures recognition,Man-machine interfaces,Wearable sensors},
number = {4},
pages = {461--482},
title = {{A survey of glove-based systems and their applications}},
volume = {38},
year = {2008}
}
@inproceedings{Grobel1997,
abstract = {This paper is concerned with the video-based recognition of isolated signs. Concentrating on the manual parameters of sign language, the system aims for the signer dependent recognition of 262 different signs. For hidden Markov modelling a sign is considered a doubly stochastic process, represented by an unobservable state sequence. The observations emitted by the states are regarded as feature vectors, that are extracted from video frames. The system achieves recognition rates up to 94{\%}},
author = {Grobel, K. and Assan, M.},
booktitle = {1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation},
doi = {10.1109/ICSMC.1997.625742},
isbn = {0-7803-4053-1},
issn = {1062-922X},
keywords = {Arm,Cameras,Computer vision,Data gloves,Deafness,Handicapped aids,Hidden Markov models,Motion analysis,Speech,Stochastic processes,doubly stochastic process,feature extraction,feature vectors,image recognition,image sequences,isolated sign language recognition,signer dependent recognition,unobservable state sequence,video frames,video-based recognition},
pages = {162--167},
title = {{Isolated sign language recognition using hidden Markov models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=625742},
volume = {1},
year = {1997}
}
@article{Premaratne2007a,
abstract = {Almost all consumer electronic equipment today uses remote controls for user interfaces. However, the variety of physical shapes and functional commands that each remote control features also raises numerous problems: the difficulties in locating the required remote control, the confusion with the button layout, the replacement issue and so on. The consumer electronics control system using hand gestures is a new innovative user interface that resolves the complications of using numerous remote controls for domestic appliances. Based on one unified set of hand gestures, this system interprets the user hand gestures into pre-defined commands to control one or many devices simultaneously. The system has been tested and verified under both incandescent and fluorescent lighting conditions. The experimental results are very encouraging as the system produces real-time responses and highly accurate recognition towards various gestures},
author = {Premaratne, P. and Nguyen, Q.},
doi = {10.1049/iet-cvi:20060198},
issn = {17519632},
journal = {IET Computer Vision},
number = {1},
pages = {35},
title = {{Consumer electronics control system based on hand gesture moment invariants}},
url = {http://digital-library.theiet.org/content/journals/10.1049/iet-cvi{\_}20060198},
volume = {1},
year = {2007}
}
@article{Zimmerman1986,
abstract = {This paper reports on the development of a hand to machine interface device that provides real-time gesture, position and orientation information. The key element is a glove and the device as a whole incorporates a collection of technologies. Analog flex sensors on the glove measure finger bending. Hand position and orientation are measured either by ultrasonics, providing five degrees of freedom, or magnetic flux sensors, which provide six degrees of freedom. Piezoceramic benders provide the wearer of the glove with tactile feedback. These sensors are mounted on the light-weight glove and connected to the driving hardware via a small cable.Applications of the glove and its component technologies include its use in conjunction with a host computer which drives a real-time 3-dimensional model of the hand allowing the glove wearer to manipulate computer-generated objects as if they were real, interpretation of finger-spelling, evaluation of hand impairment in addition to providing an interface to a visual programming language.},
author = {Zimmerman, Thomas G. and Lanier, Jaron and Blanchard, Chuck and Bryson, Steve and Harvill, Young},
doi = {10.1145/30851.275628},
isbn = {0897912136},
issn = {07366906},
journal = {ACM SIGCHI Bulletin},
keywords = {1,devices presented here,gesture recognition,human interface,i n t r,interface,motor,o d u c,t i o n,tactile interface,the,the hand gesture input,user interface},
number = {SI},
pages = {189--192},
title = {{A hand gesture interface device}},
volume = {17},
year = {1986}
}
@inproceedings{Assan1998,
abstract = {This paper is concerned with the video-based recognition of signs. Concentrating on the manual parameters of sign language, the system alms for the signer dependent recognition of 262 different signs taken from Sign Language of the Netherlands. For Hidden Markov Modelling a sign is considered a doubly stochastic process, represented by an unobservable state sequence. The observations emitted by the states are regarded as feature vectors, that are extracted from video frames. This work deals with three topics: Firstly the recognition of isolated signs, secondly the influence of variations of the feature vector on the recognition rate and thirdly an approach for the recognition of connected signs. The system achieves recognition rates up to 94{\%} for isolated signs and 73{\%} for a reduced vocabulary of connected signs.},
author = {Assan, Marcell and Grobel, Kirsti},
booktitle = {Proc. of the International Gesture Workshop on Gesture and Sign Language in Human-Computer Interaction},
isbn = {3-540-64424-5},
pages = {97--109},
title = {{Video-Based Sign Language Recognition Using Hidden Markov Models}},
year = {1998}
}
@article{Mitchell2006,
abstract = {This study traces the sources of the estimates of how many people use American Sign Language (ASL) in the United States. A variety of claims can be found in the literature and on the Internet, some of which have been shown to be unfounded but continue to be cited. In our search for the sources of the various (mis)understandings, we have found that all data-based estimates of the number of people who use ASL in the United States have their origin in a single study published in the early 1970s, which inquired about signing in general and not ASL use in particular. There has been neither subsequent research to update these estimates of the prevalence of signing nor any specific study of ASL use. The paper concludes with a call to action to rectify this problem.},
author = {Mitchell, Ross E. and Young, Travas a. and Bachleda, Bellamie and Karchmer, Michael a.},
doi = {10.1353/sls.2006.0019},
issn = {1533-6263},
journal = {Sign Language Studies},
number = {3},
pages = {306--335},
title = {{How Many People Use ASL in the United States? Why Estimates Need Updating}},
volume = {6},
year = {2006}
}
@article{Neidle1998a,
abstract = {ASL syntax makes essential use of specific non-manual expressions of syntactic features (e.g., +neg, +wh) that co-occur with manual signs. These markings occur obligatorily with manual material contained in the node of origin and optionally extend over the c-command domain of that node, thus providing important evidence for hierarchical structure. In this article, we show that agreement features, both within the clause and the noun phrase, also have non-manual correlates that exhibit the predicted distribution. Interestingly, transitive IPs and possessive DPs pattern together in their manifestation of agreement marking, while intransitive IPs pattern with non-possessive DPs.},
author = {Neidle, Carol and Bahan, Benjamin and MacLaughlin, Dawn and Lee, Robert G and Kegl, Judy},
doi = {10.1111/1467-9582.00034},
issn = {1467-9582},
journal = {Studia Linguistica},
number = {3},
pages = {191--226},
title = {{Realizations of syntactic agreement in American sign language: Similarities between the clause and the noun phrase}},
url = {http://dx.doi.org/10.1111/1467-9582.00034},
volume = {52},
year = {1998}
}
@article{Premaratne2010,
abstract = {The number of entertainment electronic apparatus used in households has dramatically increased over the years. Most of them rely on remote controllers to receive commands to perform pre-programmed functions. As most of the population in the developed world is aging, remote controllers are no longer the best interface between these entertainment units and the users. This is indeed the case due to the variety of physical shapes and functional commands that each remote control features raising numerous problems: the difficulties in locating the wanted remote control, the confusion among the button layout, the replacement issue, etc. Today, there is an opportunity for better interfaces that will find a favorable response from the aging population and the gaming enthusiasts alike. The work presented here is an extension to our recently published work [1] which received very high worldwide media publicity [2,3]. The presented system does not require the user to use long sleeved clothing as required by the previous system.},
author = {Premaratne, P and Nguyen, Q and Premaratne, M},
doi = {10.1007/978-981-4585-69-9},
isbn = {978-981-4585-68-2},
issn = {1865-0929},
journal = {Advanced Intelligent Computing Theories and Applications;93: 381-386 2010},
pages = {381--386},
title = {{Human Computer Interaction Using Hand Gestures}},
volume = {93},
year = {2010}
}
@article{Waldron1995,
abstract = {In this paper, the design and evaluation of a two-stage neural network which can recognize isolated ASL signs is given. The input to this network is the hand shape and position data obtained from a DataGlobe mounted with a Polhemus sensor. The first level consists of four backpropagation neural networks which can recognize the sign language phonology, namely, the 36 hand shapes, 10 locations, 11 orientations, and 11 hand movements. The recognized phonemes from the beginning, middle, and end of the sign are fed to the second stage which recognizes the actual signs. Both backpropagation and Kohonen's self-organizing neural work was used to compare the performance and the expandability of the learned vocabulary. In the current work, six signers with differing hand sizes signed 14 signs which included hand shape, position, and motion fragile and triple robust signs. When a backpropagation network was used for the second stage, the results show that the network was able to recognize these signs with an overall accuracy of 86{\%}. Further, the recognition results were linearly dependent on the size of the finger to the metacarpohalangeal (MP) joint and the total length of the hand. When the second stage was a Kohonen's self-organizing network, the network could not only recognize the signs with 84{\%} accuracy, but also expand its learned vocabulary through relabeling.},
author = {Waldron, Manjula B. and Kim, Soowon},
doi = {10.1109/86.413199},
isbn = {10636528 (ISSN)},
issn = {10636528},
journal = {IEEE Transactions on Rehabilitation Engineering},
number = {3},
pages = {261--271},
title = {{Isolated ASL sign recognition system for deaf persons}},
volume = {3},
year = {1995}
}
@article{Cootes1992a,
abstract = {We describe 'Active Shape Models' which iteratively adapt to refine estimates of the pose, scale and shape of models of image objects. The method uses flexible models derived from sets of training examples. These models, known as Point Distribution Models, represent objects as sets of labelled points. An initial estimate of the location of the model points in an image is improved by attempting to move each point to a better position nearby. Adjustments to the pose variables and shape parameters are calculated. Limits are placed on the shape parameters ensuring that the example can only deform into shapes conforming to global constraints imposed by the training set. An iterative procedure deforms the model example to find the best fit to the image object. Results of applying the method are described. The technique is shown to be a powerful method for refining estimates of object shape and location},
author = {Cootes, T. F. and Taylor, C. J.},
doi = {10.5244/C.6.28},
isbn = {3-540-19777-X},
journal = {Procedings of the British Machine Vision Conference 1992},
pages = {28.1--28.10},
title = {{Active Shape Models - 'Smart Snakes'}},
url = {http://www.bmva.org/bmvc/1992/bmvc-92-028.html},
year = {1992}
}
@article{Darrell1993,
abstract = {A method for learning, tracking, and recognizing human gestures using a view-based approach to model articulated objects is presented. Objects are represented using sets of view models, rather than single templates. Stereotypical space-time patterns, i.e., gestures, are then matched to stored gesture patterns using dynamic time warping. Real-time performance is achieved by using special purpose correlation hardware and view prediction to prune as much of the search space as possible. Both view models and view predictions are learned from examples. Results showing tracking and recognition of human hand gestures at over 10 Hz are presented},
author = {Darrell, T and Pentland, A},
doi = {10.1109/CVPR.1993.341109},
isbn = {0-8186-3880-X},
issn = {1063-6919},
journal = {Computer Vision and Pattern Recognition, 1993. Proceedings CVPR '93., 1993 IEEE Computer Society Conference on},
keywords = {10 Hz,Eyes,Hardware,Humans,Laboratories,Machine vision,Magnetic heads,Pattern matching,Pattern recognition,Predictive models,Statistics,articulated objects,correlation hardware,correlators,dynamic time warping,gesture learning,gesture recognition,gesture tracking,human factors,human gestures,image recognition,image sequences,motion estimation,real-time performance,real-time systems,search space pruning,space-time gestures,stereotypical space-time patterns,user interfaces,view prediction,view-based approach},
pages = {335--340},
title = {{Space-time gestures}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=341109},
volume = {1},
year = {1993}
}
@article{CooperHMCOOPER2012,
abstract = {This paper discusses sign language recognition using linguistic sub-units. It presents three types of sub-units for consideration; those learnt from appearance data as well as those inferred from both 2D or 3D tracking data. These sub-units are then combined using a sign level classifier; here, two options are presented. The first uses Markov Models to encode the temporal changes between sub-units. The second makes use of Sequential Pattern Boosting to apply discriminative feature selection at the same time as encoding temporal information. This approach is more robust to noise and performs well in signer independent tests, improving results from the 54{\%} achieved by the Markov Chains to 76{\%}.},
author = {{Cooper HMCOOPER}, Helen and {Eng-Jon Ong EONG}, Surreyacuk and {Nicolas Pugeault}, Surreyacuk and {Bowden RBOWDEN}, Richard},
doi = {10.1007/978-3-540-75773-310; Cooper, H., Bowden, R., Sign language recognition using linguistically derived sub-units (2010) Proceedings of the Language Resources and Evaluation ConferenceWorkshop on the Representation and Processing of Sign Languages : Corpora and Sign Languages Technologies, Valetta, Malta, May17-23; Elliott, R., Glauert, J., Kennaway, J., Parsons, K., (2001) D5-2: SiGML Definition, , ViSiCAST Project working document; Ershaed, H., Al-Alali, I., Khasawneh, N., Fraiwan, M., An},
file = {:Users/hadeelayoub/Desktop/LR Re-write/History/cooper12a.pdf:pdf},
isbn = {15324435 (ISSN)},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {data set,depth cameras,sequential pattern boosting,sign language recognition,signer independence,sub-units},
pages = {2205--2231},
title = {{Sign Language Recognition using Sub-Units}},
volume = {13},
year = {2012}
}
@article{Kawai1985a,
abstract = {We have developed a system which can recognize speech and generate the corresponding animation-like sign language sequence in real-time. The system is implemented in a popular personal computer. This has three video RAM (VRAM) and a voice recognition board which can recognize only the registered voice of a specific speaker. At present, forty sign language patterns and fifty finger spellings are stored on two floppy disks. Each sign pattern is composed of one to four sub-patterns. That is, if the pattern is composed of one subpattern, it is displayed as a still pattern. If not, it is displayed as a motion pattern. Japanese has fifty Kana characters. Each character has a corresponding finger spelling. Some of them are also displayed as a motion pattern composed of three sub-patterns. When a text is input from a keyboard or voice recognition circuit board (unit), a corresponding pattern sequence composed of a mixture of sign language patterns and finger spelling patterns is generated and displayed on a CRT. The data of each subpattern are read from a floppy disk and transferred to one of the VRAM (640 × 200 × 1 bits). Since the computer has three VRAM, it is possible to read and transfer the data even when a preceding pattern is still being displayed on the CRT. Thus, spoken natural language sentences can be displayed in animation form by switching the VRAM. This system will help communication between deaf-mute and healthy persons. In order to display in high speed, most programs are written in a machine language. {\textcopyright} 1985.},
author = {Kawai, H and Tamura, S},
isbn = {00313203 (ISSN)},
journal = {Pattern Recognition},
keywords = {ANIMATION-LIKE SIGN LANGUAGE SEQUENCE,Animation,DEAF-AND-MUTE SIGN LANGUAGE,FINGER SPELLINGS,INFORMATION SCIENCE,Personal computer,SIGN LANGUAGE PATTERNS,SPEECH - Recognition,Sign language,Welfare apparatus},
number = {3-4},
pages = {199--205},
title = {{Deaf-and-mute sign language generation system}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-0021976880{\&}partnerID=40{\&}md5=9fe8450d3c50307e5ed88e01e1c1fcae},
volume = {18},
year = {1985}
}
@article{Premaratne2013c,
abstract = {Dynamic hand gesture tracking and recognition system can simplify the way humans interact with computers and many other non-critical consumer electronic equipments. This system is based on the well-known "Wave Controller" technology developed at the University of Wollongong [1-3] and certainly a step forward in video gaming and consumer electronics control interfaces. Currently, computer interfacing mainly involves keyboard, mouse, joystick or gaming wheels and occasionally voice recognition for user input. These modes of interaction have constrained the artistic ability of many users, as they are required to respond to the computer through pressing buttons or moving other apparatus. Voice recognition is seen as unreliable and impractical in areas where more than one user is present. All these drawbacks can be tackled by using a reliable hand gesture tracking and recognition system based on both Lucas-Kanade and Moment Invariants approaches. This will facilitate interaction between users and computers and other consumer electronic equipments in real time. This will further enhance the user experience as users are no longer have any physical connection to the equipment being controlled. In this research, we have compared our proposed moment invariant based algorithm with template based and Fourier descriptor based methods to highlight the advantages and limitations of the proposed system. ?? 2012 Elsevier B.V.},
author = {Premaratne, Prashan and Ajaz, Sabooh and Premaratne, Malin},
doi = {10.1016/j.neucom.2011.11.039},
isbn = {09252312 (ISSN)},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Computer human interaction (HCI),Dynamic gesture recognition,Lucas-Kanade algorithm,Moment invariants,Support vector machines,Tracking},
pages = {242--249},
title = {{Hand gesture tracking and recognition system using Lucas-Kanade algorithms for control of consumer electronics}},
volume = {116},
year = {2013}
}
@book{Costello2008a,
abstract = {The Random House Webster's Unabridged American Sign Language Dictionary is a treasury of over 5,600 signs for the novice and experienced user alike. It includes complete descriptions of each sign, plus full-torso illustrations. There is also a subject index for easy reference as well as alternate signs for the same meaning.},
author = {Costello, Elaine},
booktitle = {Random House Reference},
isbn = {9780553584745},
pages = {1200},
title = {{Random House Webster's American Sign Language Dictionary}},
url = {http://books.google.com.mx/books?id=57Zrj4ELEsQC},
year = {2008}
}
@article{Vogler1998,
abstract = {We present a framework for recognizing isolated and continuous American Sign Language (ASL) sentences from three-dimensional data. The data are obtained by using physics-based three-dimensional tracking methods and then presented as input to Hidden Markov Models (HMMs) for recognition. To improve recognition performance, we model context-dependent HMMs and present a novel method of coupling three-dimensional computer vision methods and HMMs by temporally segmenting the data stream with vision methods. We then use the geometric properties of the segments to constrain the HMM framework for recognition. We show in experiments with a 53 sign vocabulary that three-dimensional features outperform two-dimensional features in recognition performance. Furthermore, we demonstrate that context-dependent modeling and the coupling of vision methods and HMMs improve the accuracy of continuous ASL recognition},
author = {Vogler, C. and Metaxas, D.},
doi = {10.1109/ICCV.1998.710744},
isbn = {81-7319-221-9},
journal = {Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)},
number = {January},
pages = {363--369},
title = {{ASL recognition based on a coupling between HMMs and 3D motion analysis}},
year = {1998}
}
@inproceedings{Starner1995b,
abstract = {Hidden Markov models (HMMs) have been used prominently and successfully in speech recognition and, more recently, in handwriting recognition. Consequently, they seem ideal for visual recognition of complex, structured hand gestures such as are found in sign language. We describe a real-time HMM-based system for recognizing sentence level American Sign Language (ASL) which attains a word accuracy of 99.2{\%} without explicitly modeling the fingers},
author = {Starner, T and Pentland, A},
booktitle = {Proceedings of the International Symposium on Computer Vision},
doi = {10.1109/ISCV.1995.477012},
isbn = {0-8186-7190-4},
keywords = {American Sign Language,American Sign Language recognition,Face recognition,Fingers,HMM-based system,Handicapped aids,Handwriting recognition,Hidden Markov models,Laboratories,Natural languages,Real time systems,Shape,Speech recognition,hand gestures,image recognition,real-time,real-time systems,sign language,visual recognition},
pages = {265--270},
title = {{Real-time American Sign Language recognition from video using hidden Markov models}},
year = {1995}
}
@article{bonvillian1981sign,
title={Sign language and autism},
author={Bonvillian, John D and Nelson, Keith E and Rhyne, Jane Milnes},
journal={Journal of Autism and Developmental Disorders},
volume={11},
number={1},
pages={125--137},
year={1981},
publisher={Springer}
}
@online{Makaton,
    author = {{The Makaton Charity}},
    title = {Let's Talk Makaton},
    urldate = {2017-11-10},
    url = {https://wetalkmakaton.org/},
}



